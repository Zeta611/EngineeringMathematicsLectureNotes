\documentclass[unfonts,oneside,a4paper]{oblivoir}

\usepackage{kotex}
\usepackage{microtype}
\usepackage{bm}

\usepackage{mathpazo}
\setmainfont{TeX Gyre Pagella}
\setmainhangulfont[ItalicFont={*},ItalicFeatures={FakeSlant=.167}]{NanumMyeongjo}
\usepackage{anyfontsize}

\usepackage{amsmath,mathtools,amssymb,amsthm,thmtools}

\theoremstyle{definition}
\newtheorem{definition}{정의}[section]

\theoremstyle{theorem}
\newtheorem{theorem}{정리}[section]

\theoremstyle{theorem}
\newtheorem{lemma}{도움정리}[section]

\theoremstyle{remark}
\newtheorem*{remark}{참고}

\theoremstyle{remark}
\newtheorem*{myremark}{나의 참고}

\theoremstyle{remark}
\newtheorem*{example}{예시}

\theoremstyle{remark}
\newtheorem*{homework}{숙제}

\renewcommand*{\proofname}{증명}

\declaretheoremstyle[
spaceabove=6pt, spacebelow=6pt,
headfont=\normalfont\itshape,
notefont=\mdseries, notebraces={(}{)},
bodyfont=\normalfont,
postheadspace=1em,
headpunct={.},
qed=$\blacksquare$,
numbered=no
]{solstyle}
\declaretheorem[style=solstyle,name=풀이]{solution}

\usepackage{diffcoeff}
\diffset[roman = true]

\usepackage{xfrac}

\renewcommand{\vec}[1]{\bm{\mathit{#1}}}
\newcommand{\vecz}{\bm{\mathrm{0}}}
\newcommand{\dd}{\mathrm{d}}
\newcommand{\dD}{\mathrm{D}}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Null}{N}
\DeclareMathOperator{\Image}{Im}
\DeclareMathOperator{\Range}{R}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\Sym}{Sym}
\DeclareMathOperator{\Vol}{Vol}
\DeclareMathOperator{\sVol}{\widetilde{Vol}}
\DeclareMathOperator{\Char}{char}

\usepackage{comment}
% \excludecomment{proof}

\makeatletter
\newcommand\nextitem[1]{%
    \setcounter{\@enumctr}{#1}%
    \addtocounter{\@enumctr}{-1}%
}
\makeatother

\title{공학 수학\\강의 노트}

\author{이재호\\\href{mailto:jaeho.lee@snu.ac.kr}{\texttt{jaeho.lee@snu.ac.kr}}}

\date{마지막 수정: \today}

\begin{document}

\maketitle

\setcounter{section}{6}
\reversemarginpar{}
\section{Linear Algebra: Matrices, Vectors, Determinants. Linear Systems}

Field는 수학과 물리학에서 지칭하는 대상이 다르다:\marginpar{\small2018.9.3.}
\begin{itemize}
    \item \textbf{체體}: 실수체, 복소수체
    \item \textbf{장場}: 전자기장, 벡터장
\end{itemize}

\begin{definition}
    체는 $+, -, \times, \div$에 대해 닫혀 있는 수 집합을 말한다.
\end{definition}

\begin{example}
    \leavevmode
    \begin{itemize}
        \item $\mathbb Q$는 조밀(dense)하다.
        \item $\mathbb R$은 꽉차(complete)있다.
        \item $\mathbb C$는 대수적으로 닫혀(closed)있다.
        \item $\mathbb Q \bigl(\sqrt 2\bigr) = \bigl\{a + b \sqrt 2\,\bigm|\,a, b \in \mathbb Q\bigr\}$
        \item $\mathbb Z_p = \{0, 1, \dots, p - 1\}$
    \end{itemize}
\end{example}

\begin{definition}
    벡터공간은 (roughly) 덧셈($+$)과 (어떤 체에 속하는) 상수배가 정의된 집합이다.
\end{definition}

\begin{example}
    \leavevmode
    \begin{itemize}
        \item $\mathbb R^2 = \left\{\begin{pmatrix} a \\ b \end{pmatrix}\,\middle|\,a, b \in \mathbb R\right\}$은 덧셈에 대해 닫혀 있고, $k \in \mathbb R$의 곱에 대해 닫혀 있으므로 실수체에 대한 벡터공간이다.
        \item $\mathbb R^n$과 $\mathbb C^n$은 실수체에 대한 벡터공간이면서 유리수체에 대한 벡터공간이다.
            이를 각각 $\mathbb Q$--벡터공간, $\mathbb R$--벡터공간이라고 한다.
        \item $\mathcal C^0_I = \{f: I \rightarrow \mathbb R \mid f \text{는 연속 함수},\ I \subset \mathbb R\}$에서는, $x \mapsto \sin x$가 하나의 벡터이다.
        \item $\mathcal C^n_I = \bigl\{f: I \rightarrow \mathbb R\,\bigm|\,\exists f^{(n)},\ f^{(n)} \text{는 연속 함수},\ I \subset \mathbb R\bigr\}$이며, $\mathcal C^0_I > \mathcal C^1_I > \dots > \mathcal C^\infty_I$이다.
    \end{itemize}
\end{example}

\setcounter{definition}{1}
\begin{definition}
    체 $F$에 대한 벡터공간 $V$의 원소 $\vec v_1, \dots, \vec v_n \in V$에 대해서,
    \begin{enumerate}
        \item $c_1, \dots, c_k \in F$일 때
            \[
                c_1 \vec v_1 + \dots + c_k \vec v_k
            \]
            는 $\vec v_1, \dots, \vec v_k$의 일차결합, 혹은 선형결합(linear combination)이라고 한다.
        \item $\vec v_1, \dots, \vec v_k$의 선형 생성
            \[
                \Span \{\vec v_1, \dots, \vec v_k\} = \left< \vec v_1, \dots, \vec v_k \right>
            \]
            은 모든 일차결합의 집합이다.
            예를 들어, 어떤 평면 상의 평행하지 않은 두 벡터는 그 평면을 선형 생성한다.
        \item $W \subset V$이면서 $W$가 벡터공간이면, $W$는 $V$의 부분공간이라고 하며 $W < V$로 표기한다.
            예를 들어, 실수 평면으로 나타내어지는 $\mathbb R^2$ 벡터공간의 부분집합인 원점을 지나는 직선은 벡터공간이므로 부분공간이다.
            반면, 원점을 지나지 않는 직선은 부분집합이지만 벡터공간은 아니므로 부분공간이 아니다.
        \item $W = \Span \{\vec v_1, \dots, \vec v_k\} < V$일 때, $\vec v_1, \dots, \vec v_k$는 $W$의 생성자(generator)라고 한다.
        \item $\vec v_1, \dots, \vec v_k$가 일차종속(linearly dependent)라는 것은 어느 하나가 다른 벡터들의 일차결합이라는 것이다.
            일차종속이 아니면 일차독립(linearly independent)이라고 한다.
            예를 들어, $\mathbb R^3$에서 $(1, 2, 3), (4, 5, 6), (7, 8, 9)$는 일차종속인 반면, $(1, 2, 3), (4, 5, 6)$은 일차독립이다.
    \end{enumerate}
\end{definition}

\begin{theorem}
    체\marginpar{\small 2018.9.5.} $F$에 대한 벡터공간 $V$의 원소들 $\vec v_1, \dots, \vec v_k$가 일차독립이라는 것은, $c_1, \dots, c_k \in F$일 때
    \[
        c_1 \vec v_1 + \dots + c_k \vec v_k = \vecz\ \Rightarrow\ c_1 = \dots = c_k = 0
    \]
    이라는 것과 동치이다.
\end{theorem}

\begin{proof}
    $(\Rightarrow)$ (재배열 가능하여) $c_k \neq 0$이라고 가정하자.
    그렇다면,
    \[
        \vec v_k = - \left(\frac{c_1}{c_k} \vec v_1 + \dots + \frac{c_{k - 1}}{c_k} \vec v_{k - 1}\right)
    \]
    이므로 $\vec v_1, \dots, \vec v_k$가 일차독립이라는 가정에 모순된다.

    $(\Leftarrow)$ $\vec v_1, \dots, \vec v_k$가 일차종속이라고 가정하자.
    즉, (재배열 가능하여) 어떤 $a_1, \dots, a_{k - 1} \in F$가 존재해서,
    \[
        \vec v_k = a_1 \vec v_1 + \dots + a_{k - 1} \vec v_{k - 1}
    \]
    이다.
    그렇다면
    \[
        a_1 \vec v_1 + \dots + a_{k - 1} \vec v_{k - 1} + (-1) \vec v_k = \vecz
    \]
    이므로 모순이다.
\end{proof}

\begin{example}
    \leavevmode
    \begin{itemize}
        \item $a, b, c \in \mathbb R$에 대해
            \[
                a(1, 2, 3) + b(4, 5, 6) + c(7, 8, 10) = (0, 0, 0)
            \]
            을 만족하는 $a, b, c$는 0밖에 없으므로 $(1, 2, 3), (4, 5, 6), (7, 8, 10)$은 일차독립이다.
        \item $\vecz, \vec v_1, \dots, \vec v_k$는 일차종속이다.
        \item $\vec v_1, \vec v_2, \vec v_3, \vec v_4$가 일차독립이면 $\vec v_1, \vec v_2, \vec v_3$ 또한 일차독립이다.
            ($\vec v_1, \vec v_2, \vec v_3$ 가 종속이면 $\vec v_3 = c_1 \vec v_1 + c_2 \vec v_2 + 0 \vec v_4$이기 때문이다.)
    \end{itemize}
\end{example}

\begin{definition}
    벡터공간 $V$의 부분공간 $W$가 일차독립인 $\vec v_1, \dots, \vec v_k$에 의해 생성될 때, $\{ \vec v_1, \dots, \vec v_k\}$를 $W$의 기저(basis)라고 한다.
\end{definition}

\begin{theorem}[차원(dimension) 정리] \label{thm:dimension}
    $W$가 유한생성된(finitely generated) (부분)공간이면 $W$의 기저의 원소의 개수는 동일하다.
    이때, 기저의 원소 개수를 $W$의 차원(dimension)이라고 하며, $\dim W$로 표기한다.
\end{theorem}

\begin{example}
    \leavevmode
    \begin{itemize}
        \item $\mathcal P_n = \left\{n\text{차 이하 실계수 다항식}\right\}$일 때, $f \in \mathcal P_n$는 항상 $\mathcal B = \{1, x, \dots, x^n\}$의 원소의 상수배의 합으로 나타낼 수 있다.
            또한,
            \[
                c_0 \cdot 1 + c_1 x + \dots + c_n x^n = 0
            \]
            이면 $c_0 = \dots c_n = 0$이므로, $\mathcal B$는 기저이다.
        \item $\mathcal P = \left\{\text{모든 다항식}\right\}$의 기저는 $\left\{1, x, x^2, \dots\right\}$이며 $\dim \mathcal{P} = \infty$이다.
    \end{itemize}
\end{example}

\begin{homework}
    \leavevmode
    \begin{enumerate}
        \item $a_1, a_2, a_3$는 서로 다른 실수이다.
            이때, $\left(1, a_1, a_1^2\right), \left(1, a_2, a_2^2\right), \left(1, a_3, a_3^2\right)$이 일차독립임을 보여라.
        \item $a_1, \dots, a_n$는 서로 다른 실수이다.
            이때, $\left(1, a_1, a_1^2\right), \dots, \left(1, a_n, a_n^2\right)$이 일차독립임을 보여라.
    \end{enumerate}
\end{homework}

\begin{theorem}\label{thm:independence}
    체 F에 대한 벡터공간 $V$의 일차독립인 $\vec v_1, \dots, \vec v_k$에 대해서, $\vec w \in V \setminus \left<\vec v_1 , \dots, \vec v_k\right>$이면 $\vec v_1, \dots, \vec v_k, \vec w$는 일차독립이다.
\end{theorem}

\begin{proof}
    $c_1, \dots, c_k, a \in F$일 때,
    \[
        c_1 \vec v_1 + \dots + c_k \vec v_k + a \vec w = \vecz
    \]
    이라고 가정하자.
    만약 $a \neq 0$이라면 
    \[
        \vec w = -\left(\frac{c_1}{a} \vec v_1 + \dots + \frac{c_k}{a} \vec v_k\right) \in \left<\vec v_1, \dots, \vec v_k\right>
    \]
    이므로 모순이다.
    따라서 $a = 0$이며, $\vec v_1, \dots, \vec v_k$는 일차독립이므로 $c_1 = \dots = c_k = a = 0$이다.
    그러므로 $\vec v_1, \dots, \vec v_k, \vec w$는 일차독립이다.
\end{proof}

정리~\ref{thm:independence}에 따르면, 일차독립인 벡터들의 선형 생성에 포함되지 않는 벡터 또한 이들과 일차독립이다.
이에 따라 어떤 유한생성된 벡터공간 $V$의 일차독립인 벡터들 $\vec v_1, \dots, \vec v_k$가 주어졌을 때, 선형 생성에 속하지 않는 벡터 $\vec w_{k + 1}, \dots, \vec w_{\dim V}$를 순차적으로 추가해서 기저를 구성할 수 있다.
이를 기저 확장(basis extension)이라고 부른다.

\begin{theorem} \label{thm:intersection}
    체 $F$에 대한 벡터공간 $V$의 부분공간 $W_1, W_2$가 주어졌을 때, $W_1 \cap W_2$ 또한 $V$의 부분공간이다.
\end{theorem}

\begin{proof}
    $\vec v_1, \vec v_2 \in W_1$이면서 $\vec v_1, \vec v_2 \in W_2$이면, 각각 벡터공간이므로 임의의 $c_1, c_2 \in F$에 대해서 $c_1 \vec v_1 + c_2 \vec v_2 \in W_1$이면서 $c_1 \vec v_1 + c_2 \vec v_2 \in W_2$이다.
    즉,
    \[
        \vec v_1, \vec v_2 \in W_1 \cap W_2\ \Rightarrow\ \forall c_1, c_2 \in F \quad c_1 \vec v_1 + c_2 \vec v_2 \in W_1 \cap W_2
    \]
    이다.
    따라서, $W_1 \cap W_2 < V$가 성립한다.
\end{proof}

\begin{definition}
    벡터공간 $V$의 부분집합 $W_1, W_2$에 대해서 $W_1 + W_2$를 다음과 같이 정의한다:
    \[
        W_1 + W_2 := \{\vec v_1 + \vec v_2 \mid \vec v_1 \in W_1, \vec v_2 \in W_2\}.
    \]
\end{definition}

\begin{example}
    좌표 평면으로 나타내어지는 벡터공간 $\mathbb R^2$의 부분집합(이면서 부분공간인)인 원점을 지나는 직선을 $W_1$이라고 하자.
    원점이 아닌 한 점만을 원소로 하는 집합 $W_2 \subset \mathbb R^2$가 주어졌을 때, $W_1 + W_2$는 $W_1$의 직선을 $W_2$의 (유일한) 원소의 점으로 평행 이동한 직선을 나타낸다.
\end{example}

\begin{theorem} \label{thm:addition}
    체 $F$에 대한 벡터공간 $V$의 부분공간 $W_1$과 $W_2$에 대해서, $W_1 + W_2$도 $V$의 부분공간이다.
\end{theorem}

\begin{proof}
    임의의 $\vec v_1 \in W_1 + W_2$에 대해 어떤 $\vec w_1 \in W_1$과 $\vec w_2 \in W_2$가 존재하며, 마찬가지로 임의의 $\vec v_2 \in W_1 + W_2$에 대해 어떤 $\vec u_1 \in W_1$과 $\vec u_2 \in W_2$가 존재한다.
    $W_1$과 $W_2$는 부분공간이므로 임의의 $c_1, c_2 \in F$에 대해
    \begin{align*}
        c_1 \vec w_1 + c_2 \vec w_2 &\in W_1\\
        c_1 \vec u_1 + c_2 \vec u_2 &\in W_2
    \end{align*}
    이고,
    \begin{equation*}
        c_1 \vec v_1 + c_2 \vec v_2 = (c_1 \vec w_1 + c_2 \vec w_2) + (c_1 \vec u_1 + c_2 \vec u_2)
    \end{equation*}
    이므로,
    \begin{equation*}
        c_1 \vec v_1 + c_2 \vec v_2 \in W_1 + W_2
    \end{equation*}
    이다.
    따라서 $W_1 + W_2 < V$이다.
\end{proof}

\begin{theorem}[그라스만(Graßmann) 공식] \label{thm:grassmann}
    체\marginpar{\small 2018.9.10.} $F$에 대한 유한생성된 벡터공간 $U$와 $W$에 대해, 다음이 성립한다:
    \begin{equation*}
        \dim (U + W) = \dim U + \dim W - \dim (U \cap W).
    \end{equation*}
\end{theorem}

\begin{proof}
    \begin{align*}
        l &= \dim (U \cap W)\\
        n &= \dim U - l\\
        m &= \dim W - l
    \end{align*}
    이라 하고, $U \cap W$의 기저를 $\mathcal B_\cap = \{\vec v_1, \dots, \vec v_l\}$라고 하자.
    $\mathcal B_\cap$을 확장해 $U$의 기저 $\mathcal B_U = \{\vec v_1, \dots, \vec v_l, \vec u_1, \dots, \vec u_n\}$와 $W$의 기저 $\mathcal B_W = \{\vec v_1, \dots, \vec v_l, \vec w_1, \dots, \vec w_m\}$을 구성할 수 있다.

    이제 $\mathcal B_U \cup \mathcal B_W = \{\vec v_1, \dots, \vec v_l, \vec u_1, \dots, \vec u_n, \vec w_1, \dots, \vec w_m\}$\footnote{이를 위해서는 $\vec u_1, \dots, \vec u_n$과 $\vec w_1, \dots, \vec w_m$에 겹치는 원소가 없어야 한다.
    이는 $\mathcal B_\cap = \mathcal B_U \cap \mathcal B_W$와 동치인데, 자명할수도 있겠지만 증명 후에 다루었다.}가 $U + W$의 기저임을 보이자.
    $U + W$의 원소 $\vec v$를 고르면, $\vec v = \vec u + \vec w$가 되는
    \[
        \vec u = a_1 \vec v_1 + \dots + a_l \vec v_l + b_1 \vec u_1 + \dots + b_n \vec u_n
    \]
    와
    \[
        \vec w = a'_1 \vec v_1 + \dots + a'_l \vec v_l + c_1 \vec w_1 + \dots + c_m \vec w_m
    \]
    가 존재하고, 이러한 $a_1, \dots, a_l, b_1, \dots, b_n, a'_1, \dots, a'_l, c_1, \dots, c_m \in F$는 유일하다.
    따라서,
    \begin{align*}
        \vec v &= \vec u + \vec w\\
               &= \left(\sum_{i = 1}^l a_i \vec v_i + \sum_{j = 1}^n b_j \vec u_j\right) + \left(\sum_{i = 1}^l a'_i \vec v_i + \sum_{k = 1}^m c_k \vec w_k\right)\\
               &= \sum_{i = 1}^l \left(a_i + a'_i\right) \vec v_i + \sum_{j = 1}^n b_j \vec u_j + \sum_{k = 1}^m c_k \vec w_k
    \end{align*}
    이므로 $\vec v$는 $\Span (\mathcal B_U \cup \mathcal B_W)$의 원소이고, $\mathcal B_U \cup \mathcal B_W$는 $U + W$를 생성한다.

    이제
    \begin{equation*}
        \sum_{i = 1}^l a_i \vec v_i + \sum_{j = 1}^n b_j \vec u_j + \sum_{k = 1}^m c_k \vec w_k = \vecz
    \end{equation*}
    이라고 하자.
    $\vec u_j$항을 제외하고 모두 우변으로 이항하면
    \begin{equation*}
        \sum_{j = 1}^n b_j \vec u_j = \sum_{i = 1}^l -a_i \vec v_i + \sum_{k = 1}^m -c_k \vec w_k
    \end{equation*}
    이 되고, $\vec z = \sum_{j = 1}^n b_j \vec u_j$라고 두자.
    \begin{align*}
        \sum_{j = 1}^n b_j \vec u_j \in \Span \mathcal B_U = U\\
        \sum_{i = 1}^l -a_i \vec v_i + \sum_{k = 1}^m -c_k \vec w_k \in \Span \mathcal B_W = W
    \end{align*}
    이므로, $\vec z \in U \cap W = \Span \mathcal B_\cap = \left<\vec v_1, \dots, \vec v_l\right>$이다.
    즉
    \[
        \vec z = d_1 \vec v_1 + \dots + d_l \vec v_l
    \]
    이 되는 $d_1, \dots, \vec d_l \in F$가 유일하게 존재한다.
    그런데 $\mathcal B_U$와 $\mathcal B_W$는 일차독립이므로 $j \in \{1, \dots, l\}$에 대해 $b_j = 0$이고 $k \in \{1, \dots, m\}$에 대해 $c_k = 0$이다.
    이에 따라 $\vec z = \vecz$이 되어 $i \in \{1, \dots, l\}$에 대해서도 $a_i = 0$이다.
    그러므로 $\mathcal B_U \cup \mathcal B_W$은 일차독립이다.

    $\mathcal B_U \cup \mathcal B_W$는 $U + W$를 생성하고 일차독립이므로, $U + W$의 기저이다.
    결국
    \begin{align*}
        \dim (U + W) &= |\mathcal B_U \cup \mathcal B_W|\\
                     &= l + n + m\\
                     &= (l + n) + (l + m) - l\\
                     &= \dim U + \dim W - \dim (U \cap W)\qedhere
    \end{align*}
\end{proof}

\begin{myremark}
    위 증명에서 $\mathcal B_\cap = \mathcal B_U \cap \mathcal B_W$임을 보이자.\footnote{참고로, $\mathcal B_U \setminus \mathcal B_W$는 $U \setminus W$의 기저가 아니다.
    $\vecz \in U \cap W$임에 따라 $\vecz \notin U \setminus W$이므로, $U \setminus W$는 벡터공간이 아니기 때문이다.}
    위 증명이 성립하기 위해서는 $\{\vec u_1, \dots, \vec u_n\}$와 $\{\vec w_1, \dots, \vec w_m\}$의 모든 원소가 달라야하기 때문이다.
    먼저 $\mathcal B_\cap \subseteq \mathcal B_U \cap \mathcal B_W$임은 자명하다.
    $\mathcal B_U \cap \mathcal B_W$에 속하는 원소 $\vec x$를 고르자.
    $\mathcal B_U \cap \mathcal B_W \subseteq U \cap W$이므로, $\vec x \in \Span \mathcal B_\cap$이다.
    나아가 $\mathcal B_U$와 $\mathcal B_W$의 원소인 $\vec x$는 $\mathcal B_\cap$의 모든 원소들과 일차독립이므로, $\vec x \in \mathcal B_\cap$이어야 한다.
    따라서 $\mathcal B_U \cap \mathcal B_W \subseteq B_\cap$이다.
\end{myremark}

\begin{definition}
    체 $F$에 대한 벡터공간 $V, W$에서, 임의의 $\vec v_1, \vec v_2 \in V$와 $c_1, c_2 \in F$에 대해
    \begin{equation*}
        L(c_1 \vec v_1 + c_2 \vec v_2) = c_1 L(\vec v_1) + c_2 L(\vec v_2)
    \end{equation*}
    을 만족하는 함수 $L: V \rightarrow W$을 선형변환(linear transformation) 혹은 선형사상(linear map)이라고 한다.
\end{definition}

\begin{example}
    \leavevmode
    \begin{enumerate}
        \item $L: \mathbb R^2 \rightarrow \mathbb R^3$에 대해,
            \begin{equation*}
                L \begin{pmatrix}
                    x \\ y
                    \end{pmatrix} = \begin{pmatrix}
                    x + 2y \\ 3x + 4y \\ 5x + 6y
                    \end{pmatrix} = \begin{pmatrix}
                    6 & 2 \\ 3 & 4 \\ 5 & 6
                    \end{pmatrix} \begin{pmatrix}
                    x \\ y
                \end{pmatrix}
            \end{equation*}
            은 선형변환이다.
        \item $L: \mathbb R^2 \rightarrow \mathbb R^3$에 대해,
            \begin{equation*}
                L \begin{pmatrix}
                    x \\ y
                    \end{pmatrix} = \begin{pmatrix}
                    x + 2y \\ 3x + 4y \\ 5x + 6y + 1
                    \end{pmatrix} = \begin{pmatrix}
                    6 & 2 \\ 3 & 4 \\ 5 & 6
                    \end{pmatrix} \begin{pmatrix}
                    x \\ y
                    \end{pmatrix} + \begin{pmatrix}
                    0 \\ 0 \\ 1
                \end{pmatrix}
            \end{equation*}
            은 선형변환이 아니다.
        \item 어떤 사상 $L: \mathbb R^n \rightarrow \mathbb R^m$이 존재한다는 것은 어떤 $m \times n$ 행렬 $A$가 존재해서 $L(\vec x) = A \vec x$라는 것이다.
        \item $f \mapsto f'$의 미분 연산자 $L: \mathcal C^\infty_I \rightarrow \mathcal C^\infty_I$는 $L(c_1 f_1 + c_2 f_2) = c_1 f'_1 + c_2 f'_2 = c_1 L(f_1) + c_2 L(f_2)$이므로 선형변환이다.
        \item $f(x) \mapsto \int_0^x f(t)\, \dd t$의 적분 연산자 $L: \mathcal C^\infty_I \rightarrow \mathcal C^\infty_I$ 또한 선형변환이다.
    \end{enumerate}
\end{example}

\begin{definition}
    두 벡터공간 $V$와 $W$ 간의 선형변환 $L: V \rightarrow W$에 대해 다음이 정의된다:
    \begin{enumerate}
        \item $L$의 핵(kernel) 혹은 영 공간(null space)\footnote{영 공간이라는 용어는 보통 행렬에 국한되어 사용되고, 핵은 추상적인 선형변환에 대해 많이 사용된다.}
            \begin{equation*}
                \ker L = \Null (L) := \{\vec v \in V \mid L(\vec v) = \vecz\}.
            \end{equation*} 
        \item $L$의 치역(range) 혹은 상(image)\footnote{치역은 정의역의 상이다. 상은 정의역의 부분집합에 대해서도 정의될 수 있다.}
            \begin{equation*}
                \Range (L) = \Image (L) := \{L(\vec v) \mid \vec v \in V\}.
            \end{equation*}
        \item $L$이 일대일 함수(one-to-one function) 혹은 단사 함수(injective function, injection)라는 것은 다음을 의미한다:
            \begin{equation*}
                \forall \vec v_1, \vec v_2 \in V \quad \bigl(L(\vec v_1) = L(\vec v_2)\ \Rightarrow\ \vec v_1 = \vec v_2\bigr)
            \end{equation*}
        \item $L$이 위로의(onto) 함수 혹은 단사 함수(surjective function, surjection)라는 것은 다음을 의미한다:
            \begin{equation*}
                \forall \vec w \in W \quad \exists \vec v \in V \quad L(\vec v) = \vec w
            \end{equation*}
    \end{enumerate}
\end{definition}

\begin{example}
    \leavevmode
    \begin{enumerate}
        \item $L \begin{pmatrix}x \\ y \\ z\end{pmatrix} = \begin{pmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9\end{pmatrix} \begin{pmatrix}x \\ y \\ z\end{pmatrix}$일 때 $\ker L\ \bigl(= \Null (L)\bigr)$을 구하시오.
            \begin{solution}
                \begin{align*}
                    &\begin{pmatrix}
                    1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9
                    \end{pmatrix}&\rightarrow &\begin{pmatrix}
                    1 & 2 & 3 \\ 0 & -3 & -6 \\ 0 & -6 & -12
                \end{pmatrix}\\
                \xrightarrow{\text{Echelon}} &\begin{pmatrix}
                    1 & 2 & 3 \\ 0 & 1 & 2 \\ 0 & 0 & 0
                    \end{pmatrix}&\xrightarrow{\text{Reduce}} &\begin{pmatrix}
                    1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0
                \end{pmatrix}
            \end{align*}
            즉
            \begin{equation*}
                \begin{pmatrix}
                    1 & 0 & -1 \\ 0 & 1 & 2 \\ 0 & 0 & 0
                    \end{pmatrix} \begin{pmatrix}
                    x \\ y \\ z
                    \end{pmatrix} = \begin{pmatrix}
                    0 \\ 0 \\ 0
                    \end{pmatrix} \ \Rightarrow \ \begin{pmatrix}
                    x \\ y \\ z
                    \end{pmatrix} = \begin{pmatrix}
                    1 \\ -2 \\ 1
                \end{pmatrix} z
            \end{equation*}
            이 되어, $\Null (L) = \left<(1, -2, 1)\right>$이다.
        \end{solution}

    \item $\Range (L)\ \bigl(= \Image(L)\bigr)$을 구하시오.
        \begin{solution}
            $L \begin{pmatrix}
                x \\ y \\ z
                \end{pmatrix} = \begin{pmatrix}
                1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9
                \end{pmatrix} \begin{pmatrix}
                x \\ y \\ z
            \end{pmatrix}$이므로,
            \begin{align*}
                L(\vec e_1) &= (1, 4, 7)\\
                L(\vec e_2) &= (2, 5, 8)\\
                L(\vec e_3) &= (3, 6, 9)
            \end{align*}
            가 된다.
            따라서
            \begin{align*}
                \Range (L) &= \{L(x, y, z) \mid (x, y, z) \in V\}\\
                           &= \{L(x \vec e_1 + y \vec e_2 + z \vec e_3) \mid (x, y, z) \in V\}\\
                           &= \{x L(\vec e_1) + y L(\vec e_2) + z L(\vec e_3) \mid (x, y, z) \in V\}\\
                           &= \Span \{L(\vec e_1), L(\vec e_2), L(\vec e_3)\}\\
                           &= \left<(1, 4, 7), (2, 5, 8), (3, 6, 9)\right>\\
                           &= \left<(1, 4, 7), (2, 5, 8)\right>
            \end{align*}
            이다.
        \end{solution}
\end{enumerate}
\end{example}

\begin{theorem} \label{thm:nullspace_range}
    벡터공간 $V$와 $W$간의 선형사상 $L: V \rightarrow W$에 대해 $\Null (L) < V$이고 $\Range (L) < W$이다.
\end{theorem}

\begin{theorem}[계수(rank)--퇴화 차수(nullity) 정리]\label{thm:rank_nullity}
    벡터공간 $V$와 $W$간의 선형사상 $L: V \rightarrow W$에 대해 다음이 성립한다:
    \begin{equation*}
        \dim \Null (L) + \dim \Range (L) = \dim V.\footnote{계수와 퇴화 차수에 대해서는 다음 강좌에 정의한다.}
    \end{equation*}
\end{theorem}

\begin{example}
    이전 예시에서 $\Null (L) = \left<(1, -2, 1)\right>$이고 $\Range (L) = \left<(1, 4, 7), (2, 5, 8)\right>$이므로, $\dim \Null (L) + \dim \Range (L) = 1 + 2 = 3$이 성립한다.
\end{example}

\begin{theorem}
    벡터\marginpar{\small 2018.9.12.} 공간 $V$와 $W$간의 선형변환 $L: V \rightarrow W$에 대해서 $\{\vec v_1, \dots, \vec v_n\}$이 $V$의 기저라면,
    \begin{equation*}
        \Range (L) = \Span \{L(\vec v_1), \dots, L(\vec v_n)\}
    \end{equation*}
    이다.
\end{theorem}

\begin{remark}
    위에서 $\{L(\vec v_1), \dots, L(\vec v_n)\}$이 기저일 필요는 없다.
\end{remark}

\begin{theorem}\label{thm:one_to_one_nullspace}
    벡터공간 $V$와 $W$간의 선형변환 $L: V \rightarrow W$가 일대일이라는 것은 $\Null (L) = \{\vecz\}$라는 것과 동치이다.
\end{theorem}

\begin{proof}
    ($\Rightarrow$) $L(\vecz) = \vecz$이고, $L$은 일대일이므로 $L(\vec v) = \vecz$를 만족하는 $\vec v = \vecz$가 유일하다.

    ($\Leftarrow$) $L(\vec v_1) = L(\vec v_2)$인 임의의 $\vec v_1, \vec v_2 \in V$를 고르자.
    그렇다면
    \begin{equation*}
        L(\vec v_1) - L(\vec v_2) = L(\vec v_1 - \vec v_2) = \vecz
    \end{equation*}
    인데, $\Null (L) = \{\vecz\}$이므로 $\vec v_1 - \vec v_2 = \vecz$이다.
    따라서 $\vec v_1 = \vec v_2$이며, $L$은 일대일 함수이다.
\end{proof}

\begin{theorem} \label{thm:one_to_one_range}
    벡터공간 $V$와 $W$간의 선형변환 $L: V \rightarrow W$가 일대일이라는 것은 $\dim \Range (L) = \dim V$라는 것과 동치이다.
\end{theorem}

\begin{proof}
    $L$이 일대일이라는 것은 정리~\ref{thm:one_to_one_nullspace}에 따라 $\Null (L) = \{\vecz\}$, 즉 $\dim \Null (L) = 0$이라는 것과 동치이다.\footnote{$\dim \Null (L) = 0\ \Leftrightarrow\ \Null (L) = \{\vecz\}$}
    그런데 정리~\ref{thm:rank_nullity}에 따라서 $\dim \Null (L) + \dim \Range (L) = \dim V$이므로, $\dim \Range(L) = \dim V$가 성립한다.
\end{proof}

\begin{theorem} \label{thm:inverse}
    벡터공간 $V$와 $W$간의 선형변환 $L: V \rightarrow W$가 전단사라는 것은 $L$의 역함수 $L^{-1}: W \rightarrow V$가 존재한다는 것과 동치이다.
    이때, $L^{-1}$도 선형이다.
\end{theorem}

\begin{proof}
    전단사 함수가 역함수를 가진다는 증명은 생략한다.
    $L^{-1}$이 선형임을 보인다.

    $W$에서 두 원소 $\vec w_1$와 $\vec w_2$를 고르자.
    이때,
    \begin{align*}
        L^{-1}(\vec w_1) &= \vec v_1\\
        L^{-1}(\vec w_2) &= \vec v_2
    \end{align*}
    라고 하자.
    그렇다면 $\vec w_1$과 $\vec w_2$의 선형결합은 다음과 같이 표현된다:
    \begin{align*}
        c_1 \vec w_1 + c_2 \vec w_2 &= c_1 L(\vec v_1) + c_2 L(\vec v_2)\\
                                &= L(c_1 \vec v_1 + c_2 \vec v_2)
    \end{align*}
    양변에 역함수를 취해주면
    \begin{align*}
        L^{-1}(c_1 \vec w_1 + c_2 \vec w_2) &= c_1 \vec v_1 + c_2 \vec v_2\\
                                     &= c_1 L^{-1}(\vec w_1) + c_2 L^{-1}(\vec w_2)
    \end{align*}
    이 되어 $L^{-1}$ 또한 선형변환임을 알 수 있다.
\end{proof}

\begin{definition}
    벡터공간 $V$와 $W$간의 선형변환 $L: V \rightarrow W$에 대해서, $L$의 계수
    \begin{equation*}
        \rank L := \dim \Range(L)
    \end{equation*}
    로 정의한다.
\end{definition}

\begin{theorem} \label{thm:one_to_one_rank}
    벡터공간 $U, V, W$에 대해 선형변환 $L_1: U \rightarrow V$와 $L_2: V \rightarrow W$이 주어졌다고 하자.
    만약 $L_2$가 일대일 함수라면, 다음이 성립한다:
    \begin{equation*}
        \rank (L_2 \circ L_1) = \rank L_1
    \end{equation*}
\end{theorem}

\begin{proof}
    정리~\ref{thm:nullspace_range}에 의해 $\Range(L_1)$은 벡터공간이므로, 선형변환 $\tilde L_2: \Range(L_1) \rightarrow W$를 정의하자:
    \begin{equation*}
        \forall \vec v \in \Range(L_1) < V \qquad \tilde L_2(\vec v) = L_2(\vec v)
    \end{equation*}
    따라서 $\tilde L_2$ 또한 일대일 선형변환임을 알 수 있다.
    정리~\ref{thm:one_to_one_range}에 따라 
    \begin{equation*}
        \dim \Range(\tilde L_2) = \dim \Range(L_1)
    \end{equation*}
    이다.
    그런데
    \begin{align*}
        \Range(\tilde L_2) &= \{\tilde L_2 (\vec v) \mid \vec v \in \Range(L_1)\}\\
                          &= \{L_2(\vec v) \mid \vec v \in \Range(L_1)\}
    \end{align*}
    이고
    \begin{align*}
        \Range(L_2 \circ L_1) &= \{(L_2 \circ L_1) (\vec u) \mid \vec u \in U\}\\
                        &= \{L_2(\vec v) \mid \vec v = L_1(\vec u),\ \vec u \in U\}\\
                        &= \{L_2(\vec v) \mid \vec v \in \Range(L_1)\}
    \end{align*}
    이므로 $\Range(\tilde L_2) = \Range(L_2 \circ L_1)$이다.
    따라서 $\dim \Range(\tilde L_2) = \dim \Range(L_2 \circ L_1) = \dim \Range(L_1)$이 성립한다.
\end{proof}

\begin{remark}
    위 증명에서 $\Range(L_2 \circ L_1)$의 기저와 $\Range(L_1)$의 기저의 개수가 같음을 보이면 되므로 $\Range(L_1)$의 기저 각각에 $L_2$를 취한 벡터들이 다시 $\Range(L_2 \circ L_1)$의 기저가 됨을 보여도 된다.
\end{remark}

\begin{definition}
    $m \times n$ 행렬(matrix)이란, $m$개의 행과 $n$개의 열로 이루어진 원소들의 나열이다:
    \begin{equation*}
        \begin{pmatrix}
            a_{11} & a_{12} & \dots & a_{1n}\\
            a_{21} & a_{22} & \dots & a_{2n}\\
            \vdots & \vdots & \ddots & \vdots\\
            a_{m1} & a_{m2} & \dots & a_{mn}
        \end{pmatrix}
    \end{equation*}
\end{definition}

두 행렬의 곱은 각각의 행렬에 대응되는 선형변환의 합성에 대응된다.
즉, 어떤 선형변환 $L_A: \mathbb R^l \rightarrow \mathbb R^m$이 $\vec x \mapsto A \vec x$로 대응시키고 $L_B: \mathbb R^m \rightarrow \mathbb R^n$이 $A \vec x \mapsto B(A \vec x) = BA \vec x$로 대응시킨다면, $BA$는 $L_B \circ L_A$에 대응된다.

$n \times n$ 행렬 $A, B$에 대해서 $AB = BA = I$이라면, $A = B^{-1}$이고 $B = A^{-1}$이다.
이때,
\begin{equation*}
    I = \begin{pmatrix}
        1 & 0 & \dots & 0\\
        0 & 1 & \dots & 0\\
        \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & \dots & 1
    \end{pmatrix}
\end{equation*}
을 말한다.

\begin{definition}
    선형변환 $L_A$에 대응되는 행렬 $A$에 대해서, 열공간(column space)은 $A$의 열벡터들의 생성이다.
    이때 열공간의 차원을 열계수(column rank)라고 부른다.
    마찬가지로, 행공간(row space)은 $A$의 행벡터들의 생성이다.
    이때 행공간의 차원을 행계수(row rank)라고 부른다.
\end{definition}

\begin{example}
    $A = \begin{pmatrix}
        1 & 2 & 3\\
        4 & 5 & 6\\
        7 & 8 & 9
    \end{pmatrix}$에서 열벡터 $(1, 4, 7)$과 $(3, 6, 9)$는 일차독립이고, $(2, 5, 8)$은 두 벡터의 합의 절반이므로 일차종속이다.
    따라서 열계수는 2이다.
    또한 행벡터 $(1, 2, 3)$과 $(7, 8, 9)$는 일차독립이고, $(4, 5, 6)$은 두 벡터의 합의 절반이므로 일차종속이다.
    따라서 행계수도 2이다.
\end{example}

\begin{theorem} \label{thm:rank_matrix}
    선형변환 $L_A$에 대응되는 행렬 $A$에 대해서, 열공간은 $\Range(L_A)$와 동일하다.
    따라서 열계수는 $L_A$의 계수 $\dim \Range(L_A) = \rank L_A$와 같다.
\end{theorem}

\begin{theorem} \label{thm:rank}
    선형변환 $L_A$에 대응되는 행렬 $A$에 대해서, 열계수와 행계수는 동일하다.
    즉, 열계수와 행계수 모두 $\rank L_A$로 나타내어지며 바로 행렬 $A$를 사용해 $\rank A$라고 쓸 수 있다.
\end{theorem}

두 행을 교환하거나, 어떤 행의 상수배를 다른 행에 더하거나, 한 행에 0이 아닌 상수를 곱하는 연산을 행렬의 기본 행 연산이라고 한다.
마찬가지로 열에 대해서도 기본 열 연산을 정의할 수 있다.

\begin{theorem} \label{thm:elementary_op}
    두 행렬 $A$와 $B$의 곱을 $C$라고 하자.
    행렬 $A$에 기본 행 연산을 취한 새로운 행렬을 $\tilde A$라고 하자.
    행렬 $C$에 동일한 기본 행 연산을 취한 새로운 행렬을 $\tilde C$라고 하면, 여전히 $\tilde A B = \tilde C$가 성립한다.
\end{theorem}

\begin{definition}
    항등행렬 $I$에 기본 행/열 연산을 한 번한 것을 기본 행렬이라고 부른다.
\end{definition}

\begin{example}
    $\begin{pmatrix}
        0 & 1 & 0\\
        1 & 0 & 0\\
        0 & 0 & 1
        \end{pmatrix}, \begin{pmatrix}
        1 & 0 & 0\\
        k & 1 & 0\\
        0 & 0 & 1
        \end{pmatrix}, \begin{pmatrix}
        k & 0 & 0\\
        0 & 1 & 0\\
        0 & 0 & 1
    \end{pmatrix}$은 모두 계수가 3인 기본 행렬들이다.
\end{example}

\begin{theorem} \label{thm:rank_elementary}
    $n \times n$ 기본 행렬의 계수는 $n$이다.
\end{theorem}

\begin{proof}
    $n \times n$ 항등행렬 $I$에 대해서, 각 열벡터를 $\vec e_1, \dots, \vec e_n$이라고 하자.
    이때 열벡터들은 서로 독립이므로 $\rank I = n$이다.

    $I$에 어떤 기본 열 연산을 하여 기본 행렬 $A$를 만들었다고 하자.
    기본 열 연산이 1. 열의 교환인 경우, 2. 열의 상수배를 하여 다른 열에 더한 경우, 3. 열에 0이 아닌 상수를 곱한 경우에 대해서 생각하자.
    \begin{enumerate}
        \item 열을 교환했을 경우\\
            $i$ 번째 열과 $j$ 번째 열을 교환했다고 하자.
            그렇다면 $A$의 열벡터들은 $\vec e_1, \dots, \vec e_j \text{($i$ 번째)}, \dots, \vec e_i \text{($j$ 번째)}, \dots, \vec e_n$이 된다.
            그런데 $\vec e_1, \dots, \vec e_n$은 순서와 상관없이 일차독립이므로 $A$의 열벡터들도 일차독립이다.
        \item 열의 상수배를 하여 다른 열에 더한 경우\\
            $i$ 번째 열에 $k$배를 하여 $j$ 열에 더했다고 하자.
            ($k \neq 0$이다.)
            그렇다면 $A$의 열벡터들은 $\vec e_1, \dots, \vec e_j + k \vec e_i, \dots, \vec e_n$이 된다.
            다음의 선형결합을 생각하자:
            \begin{equation*}
                c_1 \vec e_1 + \dots + c_j (\vec e_j + k \vec e_1) + \dots + c_n \vec e_n = \vecz
            \end{equation*}
            만약 $c_j$가 0이 아니라면, 다음과 같이 식을 재작성할 수 있다:
            \begin{equation*}
                \vec e_j = -\frac{1}{c_j} \left(c_1 \vec e_1 + \dots + c_{j - 1} \vec e_{j - 1} + c_{j + 1} \vec e_{j + 1} + \dots + c_n \vec e_n\right) + \frac{k}{c_j} \vec e_1
            \end{equation*}
            이때 $\frac{k}{c_j} \neq 0$이므로 $\vec e_1, \dots, \vec e_n$이 독립이라는 것에 모순이다.
            따라서 $c_j = 0$이다.
            결국,
            \begin{equation*}
                c_1 \vec e_1 + \dots + c_{j - 1} \vec e_{j - 1} + c_{j + 1} \vec e_{j + 1} + \dots + c_n \vec e_n = \vecz
            \end{equation*}
            가 되어, $c_1 = \dots = c_{j - 1} = c_{j + 1} = \dots = c_n = 0$이다.
            그런데 $c_j = 0$이므로 $c_1 = \dots = c_n = 0$이어서, $A$의 열벡터들도 일차독립이다.
        \item 열에 0이 아닌 상수를 곱한 경우\\
            $i$ 번째 열에 $k \neq 0$을 곱했다고 하자.
            그렇다면 $A$의 열벡터들은 $\vec e_1, \dots, k \vec e_i, \vec e_n$이 된다.
            다음의 선형결합을 생각하자:
            \begin{equation*}
                c_1 \vec e_1 + \dots + c_i (k \vec e_i) + \dots + c_n \vec e_n = \vecz
            \end{equation*}
            $\vec e_1, \dots, \vec e_n$의 독립성에 의해 $c_1 = \dots = c_i k = \dots = c_n = 0$이 된다.
            그런데 $k \neq 0$이므로 $c_1 = \dots = c_n = 0$이다.
            따라서 $A$의 열벡터들도 일차독립이다.
    \end{enumerate}
    따라서, $A$의 열공간의 차원은 $n$이고 정리~\ref{thm:rank} 의해 $\rank A = n$이다.
    마찬가지로 기본 행 연산의 경우에도 모든 과정을 행벡터에 대입하여 진행하면 동일하게 계수가 $n$임을 알 수 있다.
\end{proof}

\begin{theorem}\label{thm17}
    어떤 유한생성된 벡터공간 $V$와 부분공간 $W$가 주어졌다고 하자.
    그렇다면, 다음이 성립한다:
    \begin{equation*}
        \dim W = \dim V \Rightarrow W = V
    \end{equation*}
\end{theorem}

\begin{proof}
    $\dim W = \dim V = n$일 때, 벡터공간 $W$의 기저 $\mathcal B = \vec w_1, \dots, \vec w_n$를 구성하자.
    $\vec w_1, \dots, \vec w_n$은 일차독립이고 $V$의 원소이기도 하므로, 기저 확장을 통해 $V$의 기저를 구성할 수 있다.
    그런데 이미 $|\mathcal B| = \dim V$이므로, $\mathcal B$는 $V$의 기저이다.
    따라서 $\Span \mathcal B = W = V$이다.
\end{proof}

\begin{theorem} \label{thm:onto}
    선형변환 $L: V \rightarrow V$에 대해서, 다음의 명제는 동치이다:
    \begin{enumerate}
        \item $L$는 일대일 함수이다.
        \item $L$는 위로의 함수이다.
        \item $\rank L = \dim V$이다.
    \end{enumerate}
\end{theorem}

\begin{proof}
    정리~\ref{thm:one_to_one_range}에서 $L$가 일대일 함수라는 것은 $\dim \Range(L) = \dim V$, 즉 $\rank L = \dim V$라는 것과 동치이다.
    따라서 1과 3은 동치이다.

    이제 2와 3이 동치임을 보이자.
    만약 $L$가 위로의 함수이면, 공역과 치역이 동일하다.
    즉, $\Range(L) = V$다.
    따라서 $\rank L = \dim \Range(L) = \dim V$이 성립한다.

    역으로, $\rank L = \dim V$라고 하자.
    $\dim \Range(L) = \dim V$이므로, 정리~\ref{thm17}에 따라 $\Range(L) = V$이다.
    그러므로 $L$는 위로의 함수이다.

    따라서, 1, 2, 3은 모두 동치이다.
\end{proof}

지금까지의 내용을 간략히 요약하고, 약간의 내용을 덧붙인다.\marginpar{\small 2018.9.17.}
체란 간단히 가감승제에 대해서 닫힌 집합이고, 벡터공간은 덧셈과 스칼라 곱에 대해 닫힌 집합이다.
또한 일차결합, 일차독립, 일차종속과 기저 등에 대해서 다루었다.

정리에는 기저의 원소의 개수가 항상 일정하다는 정리~\ref{thm:dimension}), 벡터공간의 교집합도 벡터공간이라는 것을 보여주는 정리~\ref{thm:intersection}, 벡터공간의 합 또한 벡터공간이며(정리~\ref{thm:addition}) 그 차원은 각각의 차원의 합에서 교집합의 차원을 제한 것이라는 정리~\ref{thm:grassmann}이 있었다.
어떤 선형사상의 영공간과 치역은 각각 정의역과 공역의 부분공간이며(정리~\ref{thm:nullspace_range}), 그 차원의 합은 정의역의 차원임을 알려주는 정리~\ref{thm:rank_nullity}가 있었다.
또한 행렬의 행계수, 열계수, 그리고 대응되는 선형변환의 계수가 모두 같다는 정리~\ref{thm:rank}가 있었다.

\begin{theorem} \label{thm:image_independence}
    벡터공간 $V$와 $W$간의 선형변환 $L: V \rightarrow W$이 일대일 함수라는 것은 임의의 일차독립인 벡터들 $\vec v_1, \dots, \vec v_k \in V$을 골랐을 때, $L(\vec v_1), \dots, L(\vec v_k)$ 또한 일차독립이라는 것과 동치이다.
\end{theorem}

\begin{proof}
    먼저 $L$이 일대일 함수라고 가정하자.
    임의의 일차독립인 벡터들 $\vec v_1, \dots, \vec v_k$에 선형변환 $L$을 취한 벡터들의 선형결합
    \begin{equation*}
        c_1 L(\vec v_1) + \dots + c_n L(\vec v_n) = \vecz
    \end{equation*}
    를 생각하자.
    그렇다면,
    \begin{equation*}
        L(c_1 \vec v_1 + \dots + c_n \vec v_n) = \vecz
    \end{equation*}
    인데, 정리~\ref{thm:one_to_one_nullspace}에 따라 $\Null(L) = \{\vecz\}$이므로
    \begin{equation*}
        c_1 \vec v_1 + \dots + c_n \vec v_n = \vecz
    \end{equation*}
    임을 알 수 있다.
    그런데 $\vec v_1, \dots, \vec v_n$은 일차독립이므로 $c_1 = \dots = c_n = 0$이다.
    따라서 $L(\vec v_1), \dots, L(\vec v_k)$는 일차독립이다.

    이제 임의의 일차독립인 벡터들 $\vec v_1, \dots, \vec v_k$에 대해서 $L(\vec v_1), \dots, L(\vec v_k)$가 일차독립이라고 가정하자.
    $L(\vec v_1) = L(\vec v_2)$인 $\vec v_1, \vec v_2 \in V$를 고르자.
    그런데 $L(\vec v_1) = L(\vec v_2)$이므로, 가정한 명제의 대우에 따라서 $\vec v_1$과 $\vec v_2$는 일차종속이다.
    따라서
    \begin{equation*}
        c_1 \vec v_1 + c_2 \vec v_2 = \vecz
    \end{equation*}
    인 $(0, 0)$이 아닌 $(c_1, c_2)$가 존재한다.
    일반성을 잃지 않고 $c_1 \neq 0$이라고 하면,
    \begin{equation*}
        \vec v_1 = - \frac{c_2}{c_1} \vec v_2
    \end{equation*}
    가 성립한다.
    그런데 $L(\vec v_1 - \vec v_2) = \vecz$이므로,
    \begin{align*}
        L(\vec v_1 - \vec v_2) &= L\left(-\frac{c_2}{c_1} \vec v_2 - \vec v_2\right)\\
                             &= L\left(-\left(1 + \frac{c_2}{c_1}\right) \vec v_2\right)\\
                             &= -\left(1 + \frac{c_2}{c_1}\right) L(\vec v_2)\\
                             &= \vecz
    \end{align*}
    이다.
    여기서 $L(\vec v_2) = \vecz$일 경우는 모든 벡터에 대한 함숫값이 $\vecz$라는 것이고, 가정이 성립하려면 $V$와 $W$는 자명한 벡터공간 $\{\vecz\}$이어야만 한다.
    이 경우 가능한 선형변환은 유일하며, $\vecz \in V$를 $\vecz \in W$와 대응시키는 일대일(이면서 위로의) 함수이다.
    자명하지 않은 벡터공간의 경우, $\frac{c_2}{c_1} = -1$이어야 하고 따라서 $\vec v_1 = \vec v_2$이다.
    따라서 $L$은 일대일 함수이다.
\end{proof}

\begin{theorem} \label{thm:basis_span}
    체 $F$에 대한 벡터공간 $V$가 $\vec v_1, \dots, \vec v_n$에 의해 생성될 때, $\dim V \leq n$이다.
    만약 $\dim V = n$이라면 $\vec v_1, \dots, \vec v_n$은 기저를 이룬다.\footnote{강의 내용에 없었으나 본인의 필요에 따라 추가하였다.}
\end{theorem}

\begin{proof}
    $\{\vec v_1, \dots, \vec v_n\}$의 부분집합 중 일차독립인 최대인 $\{\vec v_1, \dots, \vec v_r\}$을 고르고, $\mathcal B$라고 쓰자.
    (즉 $r \leq n$이며, 원소의 순서는 재배열되었을 수 있다.)
    $V$의 임의의 원소 $\vec v$에 대해서, $V = \left<\vec v_1, \dots, \vec v_n\right>$이므로
    \begin{equation*}
        \exists c_1, \dots c_n \in F \quad \vec v = c_1 \vec v_1 + \dots + c_n \vec v_n
    \end{equation*}
    이다.
    그런데 $\vec v_{r + 1}, \dots$은 $\mathcal B$와 일차종속이므로 $\vec v_1, \dots, \vec v_r$로 다시 쓸 수 있다.
    (일차독립이라면 해당 벡터를 $\mathcal B$에 추가하여도 여전히 일차독립이어야 하는데, 그러면 $\mathcal B$가 최대인 일차독립 부분집합이라는 가정에 모순된다.)
    따라서,
    \begin{align*}
        \vec v &= c_1 \vec v_1 + \dots + c_r \vec v_r + c_{r + 1} \vec v_{r + 1} + \dots\\
               &= c_1 \vec v_1 + \dots + c_r \vec v_r + c_{r + 1} \sum_{i = 1}^r \alpha_i \vec v_i + \dots\\
               &= \bigl(c_1 + \alpha_1 (c_{r + 1} + \dots)\bigr) \vec v_1 + \dots + \bigl(c_r + \alpha_r (c_{r + 1} + \dots)\bigr) \vec v_r
    \end{align*}
    로 표현되어 $\Span \mathcal B = V$이다.
    결국 $\mathcal B$는 $V$의 기저이며, $\dim V = r \leq n$이다.

    이렇게 구성한 $\mathcal B$의 크기가 $n$이라고 하자.
    $\mathcal B \subseteq \{\vec v_1, \dots, \vec v_n\}$인데 $|\mathcal B| = \bigl|\{\vec v_1, \dots, \vec v_n\}\bigr|$이므로, $\mathcal B = \{\vec v_1, \dots, \vec v_n\}$이다.
    따라서, $\vec v_1, \dots, \vec v_n$은 기저를 이룬다.
\end{proof}

\begin{remark} \label{page:equiv_remark}
    지금까지의 내용에 따라서, 벡터공간 $V$와 $W$간의 선형변환 $L$에 대해서 다음의 명제들은 서로 동치이다:
    \begin{enumerate}
        \item $L$은 일대일 함수이다.
        \item $\Null(L) = \{\vecz\}$이다. (정리~\ref{thm:one_to_one_nullspace})
        \item $\rank L = \dim V$이다. (정리~\ref{thm:one_to_one_range})
        \item 임의의 선형독립인 $\vec v_1, \dots, \vec v_k \in V$에 대해서, $L(\vec v_1), \dots, L(\vec v_n)$도 선형독립이다. (정리~\ref{thm:image_independence})
    \end{enumerate}

    만약 $V = W$이고, $V$가 유한생성된 벡터공간이라면, 다음의 명제들도 위 명제들과 동치이다:
    \begin{enumerate}
        \nextitem{5}
        \item $L$은 위로의 함수이다. (정리~\ref{thm:onto})
        \item $L$은 일대일 대응이다. (1과 5로부터)
        \item $L$은 가역(invertible)이다. (정리~\ref{thm:inverse})
    \end{enumerate}

    나아가 $V = W = \mathbb R^n$일 경우, 선형변환 $L$에 대응되는 $n \times n$ 행렬 $A$가 있어서 다음의 명제들도 위 명제들과 동치이다:
    \begin{enumerate}
        \nextitem{8}
        \item $\forall \vec v \in \mathbb R^n \quad L^{-1}(\vec v) = A^{-1} \vec v$이다. (7로부터)
        \item $A$의 열벡터들은 일차독립이다. (정리~\ref{thm:rank}와 \ref{thm:basis_span})
    \end{enumerate}
\end{remark}

\begin{theorem}
    벡터\marginpar{\small2018.9.19.} 공간 $U, V, W$에 대해 선형변환 $L_1: U \rightarrow V$와 $L_2: V \rightarrow W$이 주어졌다고 하자.
    만약 $L_1$가 위로의 함수라면, 다음이 성립한다:
    \begin{equation*}
        \rank (L_2 \circ L_1) = \rank L_2
    \end{equation*}
\end{theorem}

\begin{proof}
    $\Range(L_2 \circ L_1)$은 $L_2 \circ L_1$에 대한 $U$의 상이다.
    즉,
    \begin{align*}
        \Range(L_2 \circ L_1) &= \{(L_2 \circ L_1)(\vec u) \mid \vec u \in U\}\\
                        &= \{L_2(\vec v) \mid \vec v = L_1(\vec u),\ \vec u \in U\}
    \end{align*}
    이다.
    그런데 $L_1$은 위로의 함수이므로
    \begin{equation*}
        \forall \vec v \in V \quad \exists \vec u \in U \quad L_1(\vec u) = \vec v
    \end{equation*}
    가 성립한다.
    따라서
    \begin{equation*}
        \{L_2(\vec v) \mid \vec v = L_1(\vec u),\ \vec u \in U\} = \{L_2(\vec v) \mid \vec v \in V\} = \Range(L_2)
    \end{equation*}
    이다.
    따라서 $\Range(L_2 \circ L_1) = \Range(L_2)$이며, $\rank(L_2 \circ L_1) = \rank L_2$이 성립함을 알 수 있다.
\end{proof}

\begin{theorem} \label{thm:elementary_op_rank}
    $m \times n$ 행렬 $A$의 계수는 기본 행/열 연산에 대해 불변이다.
\end{theorem}

\begin{proof}
    기본 행/열 연산에 대응되는 기본 행렬을 $E$라고 하고, $A$에 기본 행/열 연산을 취한 행렬을 $\tilde A$라고 하면,
    \begin{equation*}
        EA = \tilde A
    \end{equation*}
    이다.
    이때 $E$는 $m \times n$ 행렬을 $m \times n$ 행렬로 보내는 $m \times m$ 행렬이다.
    각 행렬 $E, A, \tilde A$에 대응되는 행렬을 $L_E, L_A, L_{\tilde A}$라고 하자.
    이때 $\tilde A = EA$이므로 $L_{\tilde A} = L_E \circ L_A$이다.
    정리~\ref{thm:rank_matrix}와 \ref{thm:rank_elementary}에 따라 $\rank E = \rank L_E = m$이고, 따라서 정리~\ref{thm:one_to_one_nullspace}에 따라 $L_E$는 일대일이다.
    그러므로 정리~\ref{thm:one_to_one_rank}에 의해
    \begin{equation*}
        \rank L_{\tilde A} = \rank (L_E \circ L_A) = \rank L_A
    \end{equation*}
    임을 알 수 있다.
    따라서 정리~\ref{thm:rank_matrix}에 의해 $\rank A = \rank \tilde A$이다.
\end{proof}

\begin{theorem} \label{thm:rowrankinvar}
    $m \times n$ 행렬 $A$의 행계수는 기본 행/열 연산에 대해 불변이다.
\end{theorem}

\begin{proof}
    정리~\ref{thm:rank}에 의해 자명하다.

    혹은, 좀 더 직접적으로 정리~\ref{thm:elementary_op_rank}에서 각 행렬에 전치를 취한 후, 기본 행 연산은 기본 열 연산으로, 기본 열 연산은 기본 행 연산으로 바꾼다면 행계수를 열계수로 바꾸어 생각할 수 있다.
\end{proof}

다음의 연립방정식
\begin{align*}
    x + 2y + 3z &= 6\\
    4x + 5y + 6z &= 15\\
    7x + 8y + 9z &= 24
\end{align*}
은 다음과 같이 행렬로 표현할 수 있다.
\begin{equation*}
    \begin{pmatrix}
        1 & 2 & 3\\
        4 & 5 & 6\\
        7 & 8 & 9
    \end{pmatrix}
    \begin{pmatrix}
        x\\
        y\\
        z
    \end{pmatrix}
    =
    \begin{pmatrix}
        6\\
        15\\
        24
    \end{pmatrix}
\end{equation*}
   정리~\ref{thm:elementary_op}에 의해 $\begin{pmatrix}1&2&3\\4&5&6\\7&8&9\end{pmatrix}$와 $\begin{pmatrix}6\\15\\24\end{pmatrix}$에 동일한 기본 행 연산을 하여도 등식은 성립한다.
   따라서
   \begin{equation*}
        \begin{pmatrix}
            1 & 2& 3\\
            0 & -3 & -6\\
            0 & 0 & 0
        \end{pmatrix}
        \begin{pmatrix}
            x\\
            y\\
            z
        \end{pmatrix}
        =
        \begin{pmatrix}
            6\\
            -9\\
            0
        \end{pmatrix}
   \end{equation*}
   이 된다.
   위와 같은 형태를 행사다리꼴(row echelon form)이라고 한다.
   여기서 $z$는 유일하게 정해지지 않으므로 $z = a$라고 놓으면, 후진 대입법을 통해서 $y$와 $x$를 구할 수 있다:
   \begin{equation*}
       \begin{pmatrix}
           x\\
           y\\
           z
       \end{pmatrix}
       =
       \begin{pmatrix}
           a\\
           3 - 2a\\
           a
       \end{pmatrix}
       =
       a
       \begin{pmatrix}
           1\\
           -2\\
           1
       \end{pmatrix}
       +
       \begin{pmatrix}
           0\\
           3\\
           0
       \end{pmatrix}.
   \end{equation*}
혹은 여기서 그치지 않고 다음과 같이 기본 행 연산을 더 할 수도 있다:
\begin{equation*}
    \begin{pmatrix}
        1 & 0 & -1\\
        0 & 1 & 2\\
        0 & 0 & 0
    \end{pmatrix}
    \begin{pmatrix}
        x\\
        y\\
        z
    \end{pmatrix}
    =
    \begin{pmatrix}
        0\\
        3\\
        0
    \end{pmatrix}
\end{equation*}
위와 같은 행태를 기약행 사다리꼴(reduced row echelon form)이라고 한다.
여기서 바로 $z = a$로 둔다면, 쉽게 위와 동일한 해를 구할 수 있다.\footnote{사실 동일한 표현이 나오는 것은 특수한 경우지만, 결과적으로는 동일한 해집합을 구하게 된다.}
따라서 해집합은
\begin{equation*}
    \left\{ a
    \begin{pmatrix}
        1\\
        -2\\
        1
    \end{pmatrix}
    +
    \begin{pmatrix}
        0\\
        3\\
        0
    \end{pmatrix}
    \,\middle|\, a \in \mathbb R
    \right\}
\end{equation*}
임을 알 수 있다.

\begin{theorem} \label{thm:nonhomogeneous}
    계수 행렬 $A$와 변수 열벡터 $\vec x$, 그리고 상수 열벡터 $\vec b$가 주어졌을 때, 방정식 $A \vec x = \vec b$의 해집합은
    \begin{equation*}
        \Null(L_A) + \vec x_0
    \end{equation*}
    이때, $L_A$는 행렬 $A$에 대응되는 선형변환이고, $\vec x_0$는 특수해(particular solution)이다.
\end{theorem}
\begin{proof}
    $\vec x$를 임의의 해, $\vec x_0$를 특수해라고 하자.
    즉,
    \begin{align*}
        A\vec x &= \vec b\\
        A\vec x_0 &= \vec b
    \end{align*}
    이다.
    따라서
    \begin{equation*}
        A(\vec x - \vec x_0) = 0
    \end{equation*}
    과 같이 정리할 수 있고, 정의에 따라 $\vec x - \vec x_0 \in \Null(L_A)$임을 알 수 있다.
    즉 $\vec x \in \Null(L_A) + \vec x_0$이고, 해집합은 $\Null(L_A) + \vec x_0$의 부분집합이다.

    이제 $\Null(L_A) + \vec x_0$의 한 원소 $\vec x$를 고르자.
    즉, $\Null(L_A)$에 속하는 한 원소 $\vec x_h$에 대해서
    \begin{equation*}
        \vec x = \vec x_h + \vec x_0
    \end{equation*}
    를 만족한다.
    양변에 행렬 $A$를 왼쪽에 곱해주면
    \begin{equation*}
        A \vec x = A (\vec x_h + \vec x_0) = A \vec x_h + A \vec x_0
    \end{equation*}
    이다.
    그런데 $\vec x_h \in \Null(L_A)$이므로 $A \vec x_h = \vecz$이고, $\vec x_0$는 특수해이므로 $A \vec x_0 = \vec b$이다.
    따라서
    \begin{equation*}
        A \vec x = \vecz + \vec b
    \end{equation*}
    이어서, $\vec x$는 해집합에 속한다.
    따라서 $\Null(L_A) + \vec x_0$는 해집합의 부분집합이며, 위에서 해집합 또한 $\Null(L_A) + \vec x_0$의 부분집합임을 보였으므로 둘은 같은 집합이다.
\end{proof}

\begin{example}
    방정식 $2x + y = 3$, 혹은
    \begin{equation*}
        \bigl(2 \quad 1\bigr)\,
        \begin{pmatrix}
            x\\
            y
        \end{pmatrix}
        = 3
    \end{equation*}
    의 해는 방정식 $2x + y = 0$의 해를 좌표 평면에 나타냈을 때 그려지는 직선을 평행 이동한 것이다.
    특히 특수해 $(0, 3)$, $\bigl(\frac 32, 0\bigr)$, $(1, 1)$ 등에서 하나를 골라 원래 그래프가 해당 점을 지나도록 평행 이동한 것이다.
\end{example}

정리~\ref{thm:nonhomogeneous}에서 상수 열벡터 $\vec b = \vecz$인 경우의 방정식을 동차 연립 일차 방정식(homogeneous system of linear equations), $\vec b \neq \vecz$인 경우를 비동차 연립 일차 방정(nonhomogeneous system of linear equations)라고 한다.
다음의 동립 연립 일차 방정식
\begin{equation*}
    \begin{pmatrix}
        1 & 2 & 0 & 3 & 1 & 2\\
        0 & 0 & 1 & 0 & 0 & 2\\
        0 & 0 & 0 & 1 & 0 & 1\\
        0 & 0 & 0 & 0 & 1 & 1
    \end{pmatrix}
    \begin{pmatrix}
        x_1\\
        x_2\\
        x_3\\
        x_4\\
        x_5\\
        x_6
    \end{pmatrix}
    =
    \begin{pmatrix}
        0\\
        0\\
        0\\
        0
    \end{pmatrix}
\end{equation*}
을 풀면,
\begin{equation*}
    (x_1, x_2, x_3, x_4, x_5, x_6) = a(2, 0, -2, -1, -1, 1) + b(-2, 1, 0, 0, 0, 0)\quad a, b \in \mathbb R
\end{equation*}
이 해가 된다.
이때 $a$와 $b$는 자유 변수이다.
따라서, 위 계수 행렬의 영공간은
\begin{equation*}
    \left<(2, 0, -2, -1, -1, 1), (-2, 1, 0, 0, 0, 0)\right>
\end{equation*}
임을 알 수 있다.
여기서 주목할 사실은 변수 벡터의 원소 개수가 6, 즉 차원이 6인 벡터공간에 있다는 것이고, 해공간 혹은 영공간의 차원\footnote{또는 자유변수의 개수, 즉 자유도}이 2라는 것이다.
이 둘의 차이 $6 - 2 = 4$는 계수 행렬의 계수(rank)와 같은데, 이는 우연이 아니라 계수--퇴화 차수 정리(정리~\ref{thm:rank_nullity})에 의한 것이다.

\begin{definition}
    어떤 $n \times n$ 행렬 $A$에 대해서 $n \times n$ 행렬 $B$가 존재해서
    \begin{equation*}
        A B = I
    \end{equation*}
    일 때, $A$를 가역 행렬이라 하고 $B$를 $A$의 역행렬이라고 한다.
\end{definition}

가역 행렬 $A$의 역행렬 $B$를 찾는 것은 식 $AB = I$에서 $A$를 $I$로 만드는 일련의 기본 행연산들을 $A$와 $I$에 동일하게 취하는 것이다.
$I$에 해당 기본 행연산들을 취한 결과를 $C$라고 하면, 식 $AB = I$는 $IB = C$가 된다.
따라서 $C$는 $A$의 역행렬 $B$가 됨을 알 수 있다.

이렇게\marginpar{\small2018.10.1.} 역행렬을 구할 수 있으려면 기본 행연산들을 통해 $A$를 $I$로 만들 수 있어야 한다.
그런데 정리~\ref{thm:rowrankinvar}에 따르면 기본 행연산으로 행계수가 바뀌지 않으므로, $A$의 행계수가 $A$의 행/열의 개수와 같지 않으면 $I$로 변형할 수 없다.
따라서, $A$의 역행렬이 존재할 조건은 $A$의 계수가 행/열의 개수와 같은 것이다.

\begin{definition} \label{def:multilin}
    벡터공간 $V_1, \dots, V_n, W$에 대해 어떤 함수 $f: V_1 \times \dots \times V_n \rightarrow W$가 다중선형사상(multilinear map)이라는 것은 모든 $i \in \{1, \dots, n\}$에 대한 $\vec v_i \in V_i$에 대해
    \begin{equation*}
        f(\vec v_1, \dots, c \vec v_i + \tilde c \tilde{\vec v}_i, \dots, \vec v_n) = c f(\vec v_1, \dots, \vec v_i, \dots, \vec v_n) + \tilde c f(\vec v_1, \dots, \tilde{\vec v}_i, \dots, \vec v_n)
    \end{equation*}
    을 만족한다는 것이다.
    즉, 다중선형사상은 각각의 $\vec v_i$에 대해 $f$가 선형사상이다.
    특히 $V_1 = \dots = V_n$일 때, $f$를 $n$-형식($n$-form)이라고 부른다.
\end{definition}

\begin{definition} \label{def:alternating_multilin}
    벡터공간 $V$와 $W$에 대한 다중선형사상 $f: V^n \rightarrow W$가
    \begin{equation*}
        \bigl(\exists i, j \in \{1, \dots, n\} \quad \vec v_i = \vec v_j\bigr) \quad \Rightarrow \quad f(\vec v_1, \dots, \vec v_n) = 0
    \end{equation*}
    를 만족하면 $f$를 교대다중선형사상(alternating multilinear map)이라고 부른다.
\end{definition}

\begin{theorem} \label{thm:skew_sym}
    벡터공간 $V$와 $W$에 대한 교대다중선형사상 $f: V^n \rightarrow W$은 두 벡터를 교환하면 부호가 바뀐다.
    즉,
    \begin{equation*}
        f(\vec v_1, \dots, \vec v_i, \dots, \vec v_j, \dots, \vec v_n) = -f(\vec v_1, \dots, \vec v_j, \dots, \vec v_i, \dots, \vec v_n)
    \end{equation*}
    이다.
\end{theorem}

\begin{proof}
    교대다중선형사상에서 두 벡터가 동일하면 그 함숫값은 0이므로,
    \begin{equation*}
        f(\vec v_1, \dots, \vec v_i + \vec v_j, \dots, \vec v_i + \vec v_j, \dots \vec v_n) = 0
    \end{equation*}
    이다.
    이는 다중선형성에 의해
    \begin{align*}
        &\quad f(\vec v_1, \dots, \vec v_i + \vec v_j, \dots, \vec v_i + \vec v_j, \dots \vec v_n)\\
        &= f(\vec v_1, \dots, \vec v_i, \dots, \vec v_i, \dots \vec v_n) + f(\vec v_1, \dots, \vec v_j, \dots, \vec v_j, \dots \vec v_n)\\
        &\qquad+ f(\vec v_1, \dots, \vec v_i, \dots, \vec v_j, \dots \vec v_n) + f(\vec v_1, \dots, \vec v_j, \dots, \vec v_i, \dots \vec v_n)\\
        &= f(\vec v_1, \dots, \vec v_i, \dots, \vec v_j, \dots \vec v_n) + f(\vec v_1, \dots, \vec v_j, \dots, \vec v_i, \dots \vec v_n)\\
        &= 0
    \end{align*}
    이다.
    따라서,
    \begin{equation*}
        f(\vec v_1, \dots, \vec v_i, \dots, \vec v_j, \dots \vec v_n) = -f(\vec v_1, \dots, \vec v_j, \dots, \vec v_i, \dots \vec v_n)
    \end{equation*}
    를 만족한다.
\end{proof}

\begin{example}
    교대다중선형사상 $f: \mathbb R^2 \times \mathbb R^2 \rightarrow \mathbb R$에 대해서, $f$가 $\begin{pmatrix}a & b \\ c & d\end{pmatrix}$의 열벡터를 인자로 받는다고 하자.
    이를 계산하면
    \begin{align*}
        f
        \begin{pmatrix}
            a & b\\
            c & d
        \end{pmatrix}
        &= f(a \vec e_1 + c \vec e_2, b \vec e_1 + d \vec e_2)\\
        &= ab f(\vec e_1, \vec e_1) + ad f(\vec e_1, \vec e_2) + cb f(\vec e_2, \vec e_1) + cd f(\vec e_2, \vec e_2)\\
        &= (ad - bc) f(\vec e_1, \vec e_2)\\
        &= (ad - bc) f
        \begin{pmatrix}
            1 & 0\\
            0 & 1
        \end{pmatrix}
    \end{align*}
\end{example}

\begin{definition}
    교대다중선형사상 $f: (\mathbb R^n)^n \rightarrow \mathbb R$에 대해서 $n \times n$ 단위행렬 $I$의 함숫값이 1, 즉 $f(I) = 1$이면 $f$를 행렬식(determinant)이라고 하고, $\det$라고 표기한다.
\end{definition}

행렬식을 계산할 때는 다중선형성(정의~\ref{def:multilin}) 및 교대다중선형성(정의~\ref{def:alternating_multilin}와 정리~\ref{thm:skew_sym})에 따라 각 열벡터들의 원소들을 행이 겹치지 않도록 골라서 계산할 수 있다.
예를 들어
\begin{align*}
    \det
    \begin{pmatrix}
        a & b & c\\
        d & e & f\\
        g & h & j
    \end{pmatrix}
    &= aej \det (\vec e_1, \vec e_2, \vec e_3) + bfg \det (\vec e_3, \vec e_1, \vec e_2) + cdh \det (\vec e_2, \vec e_3, \vec e_1)\\
    &\qquad+ bdj \det (\vec e_2, \vec e_1, \vec e_3) + ceg \det (\vec e_3, \vec e_2, \vec e_1) + afh \det (\vec e_1, \vec e_3, \vec e_2)\\
    &= \bigl((aej + bfg + cdh) - (bdj + ceg + afh)\bigr) \det I
\end{align*}
와 같이 계산하면 된다.

\begin{lemma}
    $\Sym\bigl(\{1, 2, \dots, n\}\bigr)$의 항등 순열(permutation) $(1\ 2\ \dots\ n)$의 원소 두 개씩을 서로 교환(transposition)하여 다시 항등 순열을 만들 때, 교환 횟수는 항상 짝수번이다.
\end{lemma}

\begin{theorem}
    $\Sym\bigl(\{1, 2, \dots, n\}\bigr)$의 순열 $\sigma_1$에서 $\sigma_2$를 만드는 교환은 홀짝성을 보존한다.
\end{theorem}

\begin{theorem} \label{thm:det_props}
    $\mathbb R^n$에\marginpar{\small 2018.10.5.} 속하는 벡터 $\vec v_1, \dots, \vec v_n$와 $n \times n$ 행렬 $A, B$에 대해서, 다음이 성립한다:
    \begin{enumerate}
        \item $\det(\vec v_1, \dots, \vec v_i, \dots, \vec v_n) = \det(\vec v_1, \dots, \vec v_i + k \vec v_j, \dots, \vec v_n)$
        \item $\det\bigl(A^t\bigr) = \det(A)$
        \item $\det(AB) = \det(A) \det(B)$ \footnote{\text{원래 수업에서는 누락되었던 부분이지만, 2018년 10월 24일 수업에 추가로 언급하신 내용이다.}}
    \end{enumerate}
\end{theorem}

\begin{theorem} \label{thm:ero_det}
    $n \times n$ 행렬 $A$의 행렬식에 대해서,
    \begin{enumerate}
        \item 두 행을 바꾸는 기본 행 연산은 행렬식의 부호를 바꾸며
        \item 어떤 행에 $k$를 곱하는 기본 행 연산은 행렬식의 값에 $k$배를 하며
        \item 어떤 행의 상수 배를 하여 다른 행에 더하는 연산에 대해 불변이다.\footnote{정리~\ref{thm:det_props}의 2에 따라, 정리~\ref{thm:ero_det}의 기본 열 연산 버전을 만들 수 있다.}
    \end{enumerate}
\end{theorem}

\begin{remark}
    행렬식을 계산할 때 다중선형성과 교대다중선형성만을 사용해 계산하면 긴 계산을 거쳐야 하지만, 위의 정리~\ref{thm:ero_det}을 사용하면 계산이 단순해질 수 있다.
    이는 어떤 행렬
    \begin{equation*}
        A =\begin{pmatrix}a&e&f&g\\0&b&h&i\\0&0&c&j\\0&0&0&d\end{pmatrix}
    \end{equation*}
    에 대해서 $\det A = abcd$이기 때문이다.
    따라서, 정리~\ref{thm:ero_det}에 따라 기본 행 연산을 통해 이와 같이 대각선 밑의 모든 원소가 0 (혹은 위의 모든 원소)인 꼴을 만들면 대각선의 곱을 통해 행렬식을 계산할 수 있다.
    이러한 꼴을 상삼각행렬 (혹은 하삼각행렬)이라고 부른다.

    일반적으로 행렬식 계산은 이렇게 상삼각행렬과 하삼각행렬의 곱으로 분해하는 과정(LU 분해)을 통해 행렬을 곱하는데 필요한 시간복잡도와 동일하게 계산할 수 있다.
\end{remark}

\begin{theorem} \label{thm:nonzero_det}
    $n \times n$ 행렬 $A$에 대응되는 선형변환 $L_A$가 일대일 함수라는 것은 $\det A$가 0이 아니라는 것과 동치이다.
\end{theorem}
\begin{proof}
    만약 행렬 $A$에서 어떤 행의 $k$배를 하여 다른 행에 더하는 기본 행 연산을 반복하였을 때 상(하)삼각행렬을 만들었다고 하자.
    상삼각행렬은 다시 어떤 행의 $k$배를 하여 다른 행에 더하는 기본 행 연산을 반복하여 (후진대입을 하는 것과 같이) 대각선 이외의 모든 원소를 0으로 만들 수 있다.
    이렇게 만들어진 행렬을 $\tilde A$라고 하자.
    이때 정리~\ref{thm:ero_det}에 따라 $\det A = \det \tilde A$이다.

    $\det \tilde A = 0$, 즉 $\tilde A$의 대각선의 한 원소가 0이라고 가정하자.
    이는 $\tilde A$의 열벡터들이 서로 독립이 아니라는 것과 동치이다. ($\tilde A$의 대각선 이외의 모든 원소들은 0이기 때문이다.)
    정리~\ref{thm:elementary_op_rank}에 따라 기본 행 연산을 통해 열벡터들의 독립성이 바뀌지 않으므로, $A$의 열벡터들도 서로 독립이 되지 않는다.
    따라서 $A$에 대응되는 $L_A$는 일대일 함수가 되지 않는다 (\pageref{page:equiv_remark}쪽의 참고 9).
\end{proof}

\begin{remark} \label{page:equiv_remark_update}
    \pageref{page:equiv_remark}쪽의 참고를 다시 옮긴다.
    벡터공간 $V$와 $W$간의 선형변환 $L$에 대해서 다음의 명제들은 서로 동치이다:
    \begin{enumerate}
        \item $L$은 일대일 함수이다.
        \item $\Null(L) = \{\vecz\}$이다. (정리~\ref{thm:one_to_one_nullspace})
        \item $\rank L = \dim V$이다. (정리~\ref{thm:one_to_one_range})
        \item 임의의 선형독립인 $\vec v_1, \dots, \vec v_k \in V$에 대해서, $L(\vec v_1), \dots, L(\vec v_n)$도 선형독립이다. (정리~\ref{thm:image_independence})
    \end{enumerate}

    만약 $V = W$이고, $V$가 유한생성된 벡터공간이라면, 다음의 명제들도 위 명제들과 동치이다:
    \begin{enumerate}
        \nextitem{5}
        \item $L$은 위로의 함수이다. (정리~\ref{thm:onto})
        \item $L$은 일대일 대응이다. (1과 5로부터)
        \item $L$은 가역(invertible)이다. (정리~\ref{thm:inverse})
    \end{enumerate}

    나아가 $V = W = \mathbb R^n$일 경우, 선형변환 $L$에 대응되는 $n \times n$ 행렬 $A$가 있어서 다음의 명제들도 위 명제들과 동치이다:
    \begin{enumerate}
        \nextitem{8}
        \item $\forall \vec v \in \mathbb R^n \quad L^{-1}(\vec v) = A^{-1} \vec v$이다. (7로부터)
        \item $A$의 열벡터들은 일차독립이다. (정리~\ref{thm:rank}와 \ref{thm:basis_span})
        \item $A \vec x = \vecz$인 $\vec x$가 $\vecz$로 유일하게 존재한다. (정리~\ref{thm:one_to_one_nullspace})
        \item $A \vec x = \vec b$인 $\vec x$가 유일하게 존재한다. (10과 정리~\ref{thm:nonhomogeneous})
        \item $\det A$가 0이 아니다. (정리~\ref{thm:nonzero_det})
    \end{enumerate}
\end{remark}

\begin{theorem} \label{thm:vec_prod_det}
    $\vec a, \vec b, \vec c \in \mathbb R^3$에 대해서 다음이 성립한다:
    \begin{equation*}
        (\vec a \times \vec b) \cdot \vec c = \vec a \cdot (\vec b \times \vec c) = \det(\vec a, \vec b, \vec c)
    \end{equation*}
\end{theorem}
\begin{proof}
    다음과 같이 놓자:
    \begin{align*}
        \vec a &= (a_1, a_2, a_3)\\
        \vec b &= (b_1, b_2, b_3)\\
        \vec c &= (c_1, c_2, c_3)
    \end{align*}
    이때,
    \begin{equation*}
        \vec a \times \vec b = (a_2 b_3 - a_3 b_2, a_3 b_1 - a_1 b_3, a_1 b_2 - a_2 b_1)
    \end{equation*}
    이며, 따라서
    \begin{align*}
        (\vec a \times \vec b) \cdot \vec c &= (a_2 b_3 - a_3 b_2) c_1 + (a_3 b_1 - a_1 b_3) c_2 + (a_1 b_2 - a_2 b_1) c_3\\
                                            &= \det(\vec c, \vec a, \vec b)\\
                                            &= \det(\vec a, \vec b, \vec c)
    \end{align*}
    이다.
    마찬가지로 $\vec a \cdot (\vec b \times \vec c)$도 성분별로 계산을 해보면 $\det(\vec a, \vec b, \vec c)$임을 알 수 있다.
\end{proof}

\begin{theorem}
    $\vec a, \vec b, \vec c \in \mathbb R^3$가 이루는 평행체의 부피 $\Vol (\vec a, \vec b, \vec c)$는 $\lvert\det(\vec a, \vec b, \vec c)\rvert$이다.
\end{theorem}
\begin{proof}
    $\vec a$와 $\vec b$가 밑면을 이룬다고 하면, 그 면적은
    \begin{equation*}
        |\vec a \times \vec b|
    \end{equation*}
    이다.
    이때 $\vec a \times \vec b$와 $\vec c$가 $\theta$의 각도를 이룬다고 하면, $\vec a, \vec b, \vec c$가 이루는 평행체의 부피는
    \begin{equation*}
        \Vol(\vec a, \vec b, \vec c) = |\vec a \times \vec b| \bigl| |\vec c| \cos \theta \bigr| = |(\vec a \times \vec b) \cdot \vec c|
    \end{equation*}
    이다.
    그런데 정리~\ref{thm:vec_prod_det}에 따라 $(\vec a \times \vec b) \cdot \vec c = \det (\vec a, \vec b, \vec c)$이므로,
    \begin{equation*}
        \Vol(\vec a, \vec b, \vec c) = \lvert\det(\vec a, \vec b, \vec c)\rvert
    \end{equation*}
    이 성립한다.
\end{proof}

\begin{definition}
    벡터 $\vec v_1, \dots, \vec v_n \in \mathbb R^n$이 양향(positively oriented)라는 것은
    \begin{equation*}
        \det(\vec v_1, \dots, \vec v_n) > 0
    \end{equation*}
    이라는 것이고, 음향(negatively oriented)이라는 것은
    \begin{equation*}
        \det(\vec v_1, \dots, \vec v_n) < 0
    \end{equation*}
    이라는 것이다.
    
    이때 $\vec v_1, \dots, \vec v_n$에 의해 생성되는 평행체의 부호수를 가지는 부피(signed volume) $\sVol(\vec v_1, \dots, \vec v_n)$는
    \begin{equation*}
        \sVol(\vec v_1, \dots, \vec v_n) = \det(\vec v_1, \dots, \vec v_n)
    \end{equation*}
    으로 정의된다.
\end{definition}

\begin{remark}
    부호수를 가지는 부피 $\sVol$는 행렬식으로 정의되었으므로, 정리~\ref{thm:det_props}와 \ref{thm:ero_det}가 그대로 적용된다:
    \begin{enumerate}
        \item $\sVol(\vec a, k \vec b, \vec c) = k \sVol(\vec a, \vec b, \vec c)$
        \item $\sVol(\vec a, \vec b + \tilde{\vec b}, \vec c) = \sVol(\vec a, \vec b, \vec c) + \sVol(\vec a, \tilde{\vec b}, \vec c)$
        \item $\sVol(\vec a, \vec b, \vec c) = -\sVol(\vec b, \vec a, \vec c)$
    \end{enumerate}
\end{remark}

\begin{definition}
    $n \times n$ 행렬 $A$에 대해서 $i$행과 $j$열을 지운 행렬 $M_{ij}$를 소행렬식(minor)이라고 부르고, $C_{ij} = (-1)^{i + j} M_{ij}$를 여인수(cofactor)이라고 한다.
\end{definition}

\begin{theorem}
    $n \times n$ 행렬 $A = (a_{ij})$에 대해서, 라플라스 전개(Laplace expansion) 혹은 여인수 전개(cofactor expansion)는 다음과 같이 행렬식을 계산하는 것을 말한다:
    \begin{equation*}
        \det A = \sum_{k = 1}^n (-1)^{i + k} a_{ik} M_{ik} = \sum_{k = 1}^n a_{ik} C_{ik}
    \end{equation*}
\end{theorem}

\begin{example}
    다음은 $3 \time 3$ 행렬의 2행을 중심으로 여인수 전개를 한 것이다:
    \begin{align*}
        \det
        \begin{pmatrix}
            a & b & c\\
            d & e & f\\
            g & h & i
        \end{pmatrix}
        &= (-1)^{2 + 1} d \det
        \begin{pmatrix}
            b & c\\
            h & j
        \end{pmatrix}
        + (-1)^{2 + 2} e \det
        \begin{pmatrix}
            a & c\\
            g & j
        \end{pmatrix}\\
        &\qquad+ (-1)^{2 + 3} f \det
        \begin{pmatrix}
            a & b\\
            g & h
        \end{pmatrix}\\
        &= -d \det
        \begin{pmatrix}
            b & c\\
            h & j
        \end{pmatrix}
        + e \det
        \begin{pmatrix}
            a & c\\
            g & j
        \end{pmatrix}
        -f \det
        \begin{pmatrix}
            a & b\\
            g & h
        \end{pmatrix}
    \end{align*}
\end{example}

\begin{theorem}
    행렬 $A$가 가역행렬일 때, 다음과 같이 역행렬을 구할 수 있다:
    \begin{equation*}
        A^{-1} = \frac{1}{\det A} (C_{ij})^t
    \end{equation*}
    이때 $(C_{ij})$는 여인수들의 행렬이다.
\end{theorem}

\section{Linear Algebra: Matrix Eigenvalue Problems}
\begin{definition}
    $\mathbb R^n$의 두 벡터 $\vec a = (a_1, \dots, a_n)$과 $\vec b = (b_1, \dots, b_n)$에 대해서 스칼라곱(scalar product), 점곱(dot product), 혹은 단순히 내적(inner product)은 다음을 말한다:
    \begin{equation*}
        \vec a \cdot \vec b = \sum_{i = 1}^n a_i b_i
    \end{equation*}
\end{definition}

\begin{theorem}
    $\mathbb R^n$의 벡터 $\vec a, \tilde{\vec a}, \vec b$와 실수 $c, \tilde c$에 대해서 다음이 성립한다:
    \begin{enumerate}
        \item $\vec a \cdot \vec b = \vec b \cdot \vec a$ (대칭성(symmetricity))
        \item $(c \vec a + \tilde c \tilde{\vec a}) \cdot \vec b = c \vec a \cdot \vec b + \tilde c \tilde{\vec a} \cdot \vec b$ (쌍선형성(bilinearlity))
        \item $\vec a \cdot \vec a \geq 0$ (등호 성립 조건은 $\vec a = \vecz$) (양의 정부호성 positive definiteness)
    \end{enumerate}
\end{theorem}

\begin{definition}
    체 $F$에 대한 벡터공간 $V$에 속하는 원소 $\vec a, \tilde{\vec a}, \vec b$와 스칼라 $c, \tilde c$에 대해, 내적(inner product) $\left<\cdot, \cdot\right>: V \times V \rightarrow F$은 다음을 말한다:
    \begin{enumerate}
        \item $\left<\vec a, \vec b\right> = \left<\vec b, \vec a\right>$ (대칭성(symmetricity))
        \item $\left<c \vec a + \tilde c \tilde{\vec a}, \vec b\right> = \left<c \vec a, \vec b\right> + \left<\tilde c \tilde{\vec a}, \vec b\right>$ (쌍선형성(bilinearlity))
        \item $\left<\vec a, \vec a\right> \geq 0$ (등호 성립 조건은 $\vec a = \vecz$) (양의 정부호성(positive definiteness))
    \end{enumerate}
    또한 내적이 정의되는 벡터공간을 내적공간(inner product space)이라고 부른다.
\end{definition}

\begin{example}
    \leavevmode
    \begin{enumerate}
        \item $\left< (a, b), (c, d) \right> = ab + cd$는 쌍선형성을 만족하지 않으므로 내적이 아니다.
        \item $\left< (a, b), (c, d) \right> = ad + bc$는 양의 정부호성을 만족하지 않으므로 내적이 아니다.
        \item $\left< (a, b, c), (d, e, f) \right> = ad + be + kcf$에서 $k = 3$일 경우 내적이고, $k = -1$일 경우 양의 정부호성을 만족하지 않으므로 내적이 아니다.
        \item 실함수 $f, g \in \mathcal C^0$에 대해서 $\left<f, g\right>_I = \int_I f g\,\dd x$는 내적이다.
        \item 실함수 $f, g \in \mathcal C^0$에 대해서 $\left<f, g\right> = \int_{-1}^1 e^x f g\,\dd x$는 내적이다. 이때 $e^x$를 $[-1, 1]$에서 내적의 무게함수(weight function)이라고 한다.
        \item 실함수 $f, g \in \mathcal C^0$에 대해서 $\left<f, g\right>_I = \int_I r f g\,\dd x$는 내적이다. 이때
            \begin{equation*}
                \forall x \in I\quad r(x) \geq 0
            \end{equation*}
            이며 $r(x) = 0$이 되는 $x$들은 서로 떨어져 있다.
    \end{enumerate}
\end{example}

\begin{theorem}
    함수 $\left<\cdot, \cdot\right>: \mathbb R^n \times \mathbb R^n \rightarrow \mathbb R$가 대칭 쌍선형형식(symmetric bilinear form)이라는 것은 벡터 $\vec a, \vec b \in \mathbb R^n$에 대해 어떤 $n \times n$ 대칭행렬 $A$가 존재해서 다음을 만족한다는 것과 동치이다:
    \begin{equation*}
        \left<\vec a, \vec b\right> = \vec b^t A \vec a
    \end{equation*}
\end{theorem}

\begin{proof}
    먼저 $\left<\cdot, \cdot\right>$가 대칭 쌍선형형식이면 $\vec a, \vec b \in \mathbb R^n$에 대해 $\left<\vec a, \vec b\right> = \vec b^t A \vec a$인 $n \times n$ 대칭행렬 $A$가 존재한다는 것을 보이자.

    $\vec a = (a_1, \dots, a_n), \vec b = (b_1, \dots, b_n)$이라고 하자.
    쌍선형성에 의해 $\left<\vec a, \vec b\right>$은 다음과 같이 정리할 수 있다:
    \begin{equation*}
        \left<\vec a, \vec b\right> = \left<a_1 \vec e_1 + \dots + a_n \vec e_n, b_1 \vec e_1 + \dots + b_n \vec e_n\right> = \sum_{i, j = 1}^n a_i b_j \left<\vec e_i, \vec e_j\right>
    \end{equation*}
    대칭성에 따라 $\left<\vec e_i, \vec e_j\right> = \left<\vec e_j, \vec e_i\right>$이며, 따라서 $A_{ij} = \left<\vec e_i, \vec e_j\right>$일 때 $A = (A_{ij}) = (A_{ji})$는 대칭행렬이다.
    이때
    \begin{align*}
        \left<\vec a, \vec b \right> &= \sum_{i, j = 1}^n a_i b_j A_{ij} = \sum_{j = 1}^n b_j \left(\sum_{i = 1}^n A_{ij} a_i\right) = \sum_{j = 1}^n b_j \left(\sum_{i = 1}^n A_{ji} a_i\right)\\
                                     &=
                                     \begin{pmatrix}
                                         b_1 & \dots & b_n
                                     \end{pmatrix}
                                     \begin{pmatrix}
                                         \sum_{i = 1}^n A_{1i} a_i\\
                                         \vdots\\
                                         \sum_{i = 1}^n A_{ni} a_i
                                     \end{pmatrix}\\
                                     &=
                                     \begin{pmatrix}
                                         b_1 & \dots & b_n
                                     \end{pmatrix}
                                     \begin{pmatrix}
                                         A_{11} & \dots & A_{1n}\\
                                         \vdots & \ddots & \vdots\\
                                         A_{n1} & \dots & A_{nn}
                                     \end{pmatrix}
                                     \begin{pmatrix}
                                         a_1\\
                                         \vdots\\
                                         a_n
                                     \end{pmatrix}
    \end{align*}
    이므로 대칭행렬 $A$에 대해 $\left<\vec a, \vec b\right> = \vec b^t A \vec a$임을 알 수 있다.

    이제 대칭행렬 $A$에 대해 $\vec a, \vec b \in \mathbb R^n$일 때 $\left<\vec a, \vec b\right> = \vec b^t A \vec a$이면 $\left<\cdot, \cdot\right>$이 대칭 쌍선형형식임을 보이자.

    $\vec a = (a_1, \dots, a_n), \vec b = (b_1, \dots, b_n)$이라고 하자.
    그러면
    \begin{align*}
        \left<\vec a, \vec b\right> &= \vec b^t A \vec a\\
                                    &=
                                    \begin{pmatrix}
                                        b_1 & \dots & b_n
                                    \end{pmatrix}
                                    \begin{pmatrix}
                                        A_{11} & \dots & A_{1n}\\
                                        \vdots & \ddots & \vdots\\
                                        A_{n1} & \dots & A_{nn}
                                    \end{pmatrix}
                                    \begin{pmatrix}
                                        a_1\\
                                        \vdots\\
                                        a_n
                                    \end{pmatrix}\\
                                    &= \sum_{i, j = 1}^n a_i b_j A_{ij} = \sum_{i = 1}^n a_i \left(\sum_{j = 1}^n A_{ij} b_j\right)\\
                                    &=
                                    \begin{pmatrix}
                                        a_1 & \dots & a_n
                                    \end{pmatrix}
                                    \begin{pmatrix}
                                        A_{11} & \dots & A_{1n}\\
                                        \vdots & \ddots & \vdots\\
                                        A_{n1} & \dots & A_{nn}
                                    \end{pmatrix}
                                    \begin{pmatrix}
                                        b_1\\
                                        \vdots\\
                                        b_n
                                    \end{pmatrix}\\
                                    &= \vec a^t A \vec b = \left<\vec b, \vec a\right>
    \end{align*}
    이므로 대칭성을 만족한다.
    또한 벡터 $\tilde{\vec a} \in \mathbb R^n$을 $(\tilde a_1, \dots, \tilde a_n)$으로 놓고 스칼라 $c, \tilde c \in \mathbb R$에 대해서 쌍선형성을 만족한다:
    \begin{equation*}
        \left<c \vec a + \tilde c \tilde{\vec a}, \vec b\right> = \vec b^t A (c \vec a + \tilde c \tilde{\vec a}) = c \vec b^t A \vec a + \tilde c \vec b^t A \tilde{\vec a}
    \end{equation*}

    따라서 $\left<\cdot, \cdot\right>$이 쌍선형형식이라는 것과 $\left<\vec a, \vec b\right> = \vec b^t A \vec a$인 대칭행렬 $A$가 존재한다는 것은 동치이다.
\end{proof}

\begin{example}
    두 벡터 $(a, b, c)$와 $(d, e, f)$에 대한 내적 $ae + bd + 4cf$에 대해서,
    \begin{equation*}
        \left<(a, b, c), (d, e, f)\right> = ae + bd + 4cd =
        \begin{pmatrix}
            d & e & f
        \end{pmatrix}
        \begin{pmatrix}
            0 & 1 & 0\\
            1 & 0 & 0\\
            0 & 0 & 4
        \end{pmatrix}
        \begin{pmatrix}
            a \\ b \\ c
        \end{pmatrix}
    \end{equation*}
    이다.
\end{example}

\begin{definition}
    내적공간에 속하는 벡터 $\vec v$에 대해
    \begin{equation*}
        \lVert \vec v \rVert = \sqrt{\left<\vec v, \vec v\right>}
    \end{equation*}
    를 $\vec v$의 노름(norm)이라고 한다.
\end{definition}

\begin{theorem} [코시-부냐콥스키-슈바르츠(Cauchy-Bunyakovsky-Schwarz) 부등식] \label{thm:CBS}
    내적공간에 속하는 두 벡터 $\vec a$와 $\vec b$에 대해 다음 부등식이 성립한다:
    \begin{equation*}
        \bigl|\left<\vec a, \vec b\right>\bigr| \leq \sqrt{\left<\vec a, \vec a\right>} \sqrt{\left<\vec b, \vec b\right>} = \lVert \vec a \rVert \lVert \vec b \rVert
    \end{equation*}
    등호 성립 조건은 $\vec a$와 $\vec b$가 일차종속일 경우이다.
    위 부등식을 코시-부냐콥스키-슈바르츠(Cauchy-Bunyakovsky-Schwarz) 부등식 혹은 CBS 부등식이라고 한다.
\end{theorem}

\begin{proof}
    벡터 $\vec a$와 $\vec b$에 대해 다음과 같은 함수 $f$를 정의하자:
    \begin{equation*}
        f(t) = \left<\vec a + t \vec b, \vec a + t \vec b\right>
    \end{equation*}
    내적은 양의 정부호성을 가지므로 $f(t) \geq 0$임을 알 수 있다.
    또한 내적의 쌍선형성과 대칭성에 따라 $f(t)$는 다음과 같이 정리할 수 있다:
    \begin{equation*}
        f(t) = \left<\vec b, \vec b\right> t^2 + 2 \left<\vec a, \vec b\right> t + \left<\vec a, \vec a\right>
    \end{equation*}
    따라서 $f(t)$는 $t$에 관한 이차함수이며, 항상 0 이상이므로 판별식이 $\sfrac D4 \leq 0$를 만족한다.
    즉,
    \begin{equation*}
       \left<\vec a, \vec b\right>^2 \leq \left<\vec a, \vec a\right> \left<\vec b, \vec b\right>
    \end{equation*}
    이다.
\end{proof}

\begin{definition}
    내적공간의 두 벡터 $\vec a$와 $\vec b$가 이루는 각도 $\theta$를 다음과 같이 정의한다:
    \begin{equation*}
        \cos \theta = \frac{\left<\vec a, \vec b\right>}{\lVert \vec a\rVert \lVert \vec b \rVert}
    \end{equation*}
    이때 $-1 \leq \frac{\left<\vec a, \vec b\right>}{\lVert \vec a\rVert \lVert \vec b \rVert} \leq 1$임은 정리~\ref{thm:CBS}의 CBS 부등식에 의해 보장된다.
\end{definition}

\begin{definition}
    내적공간에 속하는 두 벡터 $\vec a$와 $\vec b$가 서로 수직이라는 것은 $\left<\vec a, \vec b\right> = 0$이라는 것이다.
    나아가 $\vec a$와 $\vec b$가 단위 벡터이면 둘이 정규직교(orthonormal)한다고 한다.

    벡터들의 집합 $S$의 모든 원소들이 서로 직교하면 $S$를 직교 집합(orthogonal set), 서로 정규직교하면 $S$를 정규직교 집합(orthonormal set)이라고 부른다.
\end{definition}

\begin{theorem}
    내적공간에 속하는 정규직교 집합은 일차독립이다.
\end{theorem}

\begin{proof}
    내적공간 $V$에 속하는 벡터들 $\vec a_1, \dots, \vec a_n$의 집합이 정규직교 집합이라고 하자.
    이때 $\vec a_i$들의 선형결합
    \begin{equation*}
        c_1 \vec a_1 + \dots + c_n \vec a_n = 0
    \end{equation*}
    의 양변에 $i \in \{1, \dots, n\}$인 $\vec a_i$를 내적하면
    \begin{equation*}
        \left<(c_1 \vec a_1 + \dots + c_n \vec a_n), \vec a_i\right> = \sum_{j \in \{1, \dots, n\} \setminus \{i\}} c_j \left<\vec a_j, \vec a_i\right> + c_i \left<\vec a_i, \vec a_i\right>
    \end{equation*}
    인데, 모든 벡터들은 서로 직교하므로 $i \neq j$이면 $\left<\vec a_j, \vec a_i\right> = 0$이다.
    나아가 모든 벡터들은 단위 벡터이므로 $\left<\vec a_i, \vec a_i\right> = 1$이다.
    따라서
    \begin{equation*}
        \left<(c_1 \vec a_1 + \dots + c_n \vec a_n), \vec a_i\right> = c_i = 0
    \end{equation*}
    이며, $c_i = 0$이 된다.
    임의의 $i \in \{1, \dots, n\}$에 대해서 위 과정을 반복하면, 모든 $i$에 대해서 $c_i = 0$임을 알 수 있다.
    따라서 $\vec a_1, \dots, \vec a_n$은 일차독립이다.
\end{proof}

\begin{definition}
    내적공간 $V$와 $V$의 부분공간 $W$에 대해서, $\vec a \in V$를 잡자.
    이때 $\vec a$의 $W$ 위로의 정사영 $\vec v$는 
    \begin{equation*}
        \forall \vec w \in W \qquad \left<\vec a - \vec v, \vec w\right> = 0
    \end{equation*}
    을 만족한다.
    이때 $\vec v = p_W (\vec a)$로 표기한다.

    $\vec a \in V$의 어떤 벡터 $\vec u \in V$가 생성하는 벡터공간 $\left<\vec u\right>$로의 정사영을 $p_{\vec u} (\vec a)$로도 표기한다.
\end{definition}

\begin{theorem}
    내적공간 $V$와 부분공간 $W$에 대해서 $W$의 직교 기저가 $\{\vec w_1, \dots, \vec w_n\}$일 때, $\vec v \in V$의 $W$ 위로의 정사영 $p_W (\vec v)$는 다음과 같다:
    \begin{equation*}
        p_W (\vec v) = \frac{\left<\vec v, \vec w_1\right>}{\left<\vec w_1, \vec w_1\right>} + \dots + \frac{\left<\vec v, \vec w_n\right>}{\left<\vec w_n, \vec w_n\right>}
    \end{equation*}
\end{theorem}

\begin{proof}
    $\vec v \in V$의 $W$ 위로의 정사영 $p_W (\vec v)$는 $W$의 원소이므로, $W$의 직교 기저 $\{\vec w_1, \dots \vec w_n\}$의 선형결합으로 표현할 수 있다:
    \begin{equation*}
        p_W (\vec v) = c_1 \vec w_1 + \dots + c_n \vec w_n
    \end{equation*}
    그리고 정사영의 정의에 따라
    \begin{equation*}
        \forall i \in \{1, \dots, n\} \qquad \left<\vec v - p_W (\vec v), \vec w_i\right> = 0
    \end{equation*}
    를 만족한다.
    정리하면,
    \begin{align*}
        \left<\vec v - p_W (\vec v), \vec w_i\right> &= \left<\vec v, \vec w_i\right> - \left<p_W (\vec v), \vec w_i\right>\\
                                                     &= \left<\vec v, \vec w_i \right> - \left<c_1 \vec w_1 + \dots + c_n \vec w_n, \vec w_i\right>\\
                                                     &= \left<\vec v, \vec w_i \right> - \sum_{j \in \{1, \dots, n\} \setminus \{i\}} c_j \left<\vec w_j, \vec w_i\right> - c_i \left<\vec w_i, \vec w_i\right>\\
                                                     &= \left<\vec v, \vec w_i \right> - c_i \left<\vec w_i, \vec w_i\right> = 0
    \end{align*}
    이므로
    \begin{equation*}
        c_i = \frac{\left<\vec v, \vec w_i\right>}{\left<\vec w_i, \vec w_i\right>}
    \end{equation*}
    이다.
    따라서 정사영 $p_W (\vec v)$는
    \begin{equation*}
        p_W (\vec v) = \frac{\left<\vec v, \vec w_1\right>}{\left<\vec w_1, \vec w_1\right>} + \dots + \frac{\left<\vec v, \vec w_n\right>}{\left<\vec w_n, \vec w_n\right>}
    \end{equation*}
    이다.
\end{proof}

\begin{theorem} [그람-슈미트(Gram-Schmidt) 과정]
    내적공간 $W$의 기저 $\{\vec v_1, \dots, \vec v_n\}$에 대해서, 다음의 과정을 그람-슈미트(Gram-Schmidt) 과정이라고 부른다:
    \begin{align*}
        \vec w_1 &= \vec v_1\\
        \vec w_2 &= \vec v_2 - p_{\vec w_1} (\vec v_2)\\
        \vec w_3 &= \vec v_3 - p_{\vec w_1} (\vec v_3) - p_{\vec w_2} (\vec v_3)\\
                 &\vdots\\
        \vec w_n &= \vec v_n - \sum_{i = 1}^{n - 1} p_{\vec w_i} (\vec v_n)
    \end{align*}
    이렇게 구한 $\vec w_1, \dots, \vec w_n$의 집합은 $W$의 직교 기저이다.
\end{theorem}

\begin{example}
    \leavevmode
    \begin{enumerate}
        \item $\mathbb R^3 = \langle (1, 0, 0), (0, 1, 1), (1, 2, 3) \rangle$와 스칼라곱으로 주어진 내적에 그람-슈미트 과정을 다음과 같이 실행한다:
            \begin{align*}
                \vec w_1 &= (1, 0, 0)\\
                \vec w_2 &= (0, 1, 1) - \frac{(1, 0, 0) \cdot (0, 1, 1)}{(1, 0, 0) \cdot (1, 0, 0)} (1, 0, 0) = (0, 1, 1)\\
                \vec w_3 &= (1, 2, 3) - \frac{(1, 0, 0) \cdot (1, 2, 3)}{(1, 0, 0) \cdot (1, 0, 0)}(1, 0, 0) - \frac{(0, 1, 1) \cdot (1, 2, 3)}{(0, 1, 1) \cdot (0, 1, 1)}(0, 1, 1)\\
                         &= (1, 2, 3) - (1, 0, 0) - \frac 52 (0, 1, 1) = \left(0, -\frac 12, \frac 12\right)
            \end{align*}
            따라서 $\mathbb R^3$는 직교 기저 $\left\{(1, 0, 0), (0, 1, 1), \left(0, -\frac12, \frac12\right)\right\}$로 생성된다.
        \item 벡터공간 $W = \left< 1, x, x^2 \right>$에 대해서 $I = [-1, 1]$이고 $r(x) = 1$인 내적 $\langle \cdot, \cdot \rangle_{I, r(x)}$가 주어졌다고 하자.
            즉,
            \begin{equation*}
                \langle f, g \rangle = \int_{-1}^1 f(x) g(x)\,\dd x
            \end{equation*}
            이다.
            이때 $\{1, x, x^2\}$에 대해서 그람-슈미트 과정을 다음과 같이 실행한다:
            \begin{align*}
                \vec w_1 &= 1\\
                \vec w_2 &= x - \frac{\langle 1, x \rangle}{\langle 1, 1\rangle} 1 = x - \frac{\left. \frac{x^2}{2}\right|_{-1}^1}{\left. x\right|_{-1}^1} = x\\
                \vec w_3 &= x^2 - \frac{\langle x^2, 1 \rangle}{\langle 1, 1\rangle} 1 - \frac{\langle x^2, x\rangle}{\langle x, x \rangle} x = x^2 - \frac{\left.\frac{x^3}{3}\right|_{-1}^1}{\left. x \right|_{-1}^1} - \frac{\left. \frac{x^4}{4} \right|_{-1}^1}{\left. \frac{x^3}{3} \right|_{-1}^1} x = x^2 - \frac13
            \end{align*}
            따라서 $\left< 1, x, x^2 \right>$는 직교 기저 $\left\{1, x, x^2 - \frac13\right\}$로 생성된다.
        \item 벡터공간 $W = \left< 1, x, x^2, x^3 \right>$에 대해서 $I = [-1, 1]$이고 $r(x) = 1$인 내적 $\langle \cdot, \cdot \rangle_{I, r(x)}$가 주어졌다고 하자.
            이때 $\{1, x, x^2, x^3\}$에 대해서 그람-슈미트 과정을 실행할 때, 세 번째 직교 기저 벡터까지는 위에서 $\{1, x, x^2\}$에 대해서 그람-슈미트 과정을 실행한 것과 동일하다:
            \begin{align*}
                \vec w_1 &= 1\\
                \vec w_2 &= x\\
                \vec w_3 &= x^2 - \frac13\\
                \vec w_4 &= x^3 - \frac{\langle x^3, 1 \rangle}{\langle 1, 1\rangle} 1 - \frac{\langle x^3, x\rangle}{\langle x, x \rangle} x - \frac{\left\langle x^3, x^2 - \frac13\right\rangle}{\left\langle x^2 - \frac13, x^2 - \frac13 \right\rangle} \left(x^2 - \frac13\right)\\
                         &= x^3 - \frac{\left.\frac{x^4}{4}\right|_{-1}^1}{\left. x \right|_{-1}^1} - \frac{\left. \frac{x^5}{5} \right|_{-1}^1}{\left. \frac{x^3}{3} \right|_{-1}^1} x - \frac{\left. \frac{x^6}{6} - \frac{x^4}{12} \right|_{-1}^1}{\left. \frac{x^5}{5} - \frac{2x^3}{9} + \frac x9 \right|_{-1}^1}\left(x^2 - \frac13\right)\\
                         &= x^3 - \frac{3}{5}x
            \end{align*}
            따라서 $\left< 1, x, x^2, x^3 \right>$는 직교 기저 $\left\{1, x, x^2 - \frac13, x^3 - \frac{3}{5}x\right\}$로 생성된다.
    \end{enumerate}
\end{example}

\begin{theorem} \label{thm:orthogonal_mat}
    $n \times n$ 행렬 $A$와 $\mathbb R^n$에 스칼라곱이 주어져 있을 때, 다음의 명제들은 동치이다:
    \begin{enumerate}
        \item $A^t A = I$
        \item $A A^t = I$
        \item $\forall \vec x \in \mathbb R^n \quad \lVert A \vec x \rVert = \lVert \vec x \rVert$
        \item $\forall \vec x, \vec y \in \mathbb R^n \quad A \vec x \cdot A \vec y = \vec x \cdot \vec y$
        \item $A$의 행벡터들이 정규직교한다.
        \item $A$의 열벡터들이 정규직교한다.
    \end{enumerate}
\end{theorem}

\begin{proof}
    먼저 1과 2가 동치임은 $A^t = A^{-1}$이기에 자명하다.
    1, 2와 5, 6이 동치임을 보이자.

    $n \times n$ 행렬 $A$의 열벡터들을 $\vec a_1, \dots, \vec a_n$, 행벡터들을 $\vec b_1, \dots, \vec b_n$이라 하자.
    즉,
    \begin{equation*}
        A =
        \begin{pmatrix}
            \vec a_1 & \dots & \vec a_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            \vec b_1 \\ \vdots \\ \vec b_n
        \end{pmatrix}
    \end{equation*}
    이다.
    그렇다면 $A$의 전치행렬 $A^t$는
    \begin{equation*}
        A^t =
        \begin{pmatrix}
            \vec a_1^t \\ \vdots \\ \vec a_n^t
        \end{pmatrix}
        =
        \begin{pmatrix}
            \vec b_1^t & \dots & \vec b_n^t
        \end{pmatrix}
    \end{equation*}
    이다.
    따라서 $A$와 $A^t$의 곱은 다음과 같다:
    \begin{align*}
        &A^tA =
        \begin{pmatrix}
            \vec a_1^t \\ \vdots \\ \vec a_n^t
        \end{pmatrix}
        \begin{pmatrix}
            \vec a_1 & \dots & \vec a_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            \vec a_1 \cdot \vec a_1 & \dots & \vec a_1 \cdot \vec a_n\\
            \vdots & \ddots & \vdots\\
            \vec a_n \cdot \vec a_1 & \dots & \vec a_n \cdot \vec a_n\\
        \end{pmatrix}
        \\
        = &AA^t =
        \begin{pmatrix}
            \vec b_1 \\ \vdots \\ \vec b_n
        \end{pmatrix}
        \begin{pmatrix}
            \vec b_1^t & \dots & \vec b_n^t
        \end{pmatrix}
        =
        \begin{pmatrix}
            \vec b_1 \cdot \vec b_1 & \dots & \vec b_1 \cdot \vec b_n\\
            \vdots & \ddots & \vdots\\
            \vec b_n \cdot \vec b_1 & \dots & \vec b_n \cdot \vec b_n\\
        \end{pmatrix}
    \end{align*}
    따라서 $A^t A$ 혹은 $AA^t$가 $I$라는 것은
    \begin{equation*}
        \vec a_i \cdot \vec a_j = \vec b_i \cdot \vec b_j = \delta_{ij}
    \end{equation*}
    이라는 것과 동치이다.
    이때 $\delta_{ij}$는 크로넥커(Kronecker) 델타이며, 이는 $i = j$일 때 1, $i \neq j$일 때 0을 가지는 함수이다.
    그러므로 1, 2와 5, 6은 동치이다.

    3과 4가 동치임을 보이자.
    4에서 $\vec x = \vec y$인 경우에 3이 되는 것은 자명하다.
    따라서 3에서 4가 성립함을 보이자.

    임의의 $\vec x \in \mathbb R^n$에 대해서 $\lVert A \vec x \rVert = \lVert \vec x \rVert$라고 하자.
    벡터 $\vec a, \vec b \in \mathbb R^n$을 고르면,
    \begin{equation*}
        \lVert A (\vec a + \vec b) \rVert = \lVert \vec a + \vec b \rVert
    \end{equation*}
    가 성립한다.
    이때
    \begin{align*}
        \lVert A (\vec a + \vec b) \rVert &= \sqrt{A (\vec a + \vec b) \cdot A (\vec a + \vec b) }\\
                                          &= \sqrt{(A \vec a + A \vec b) \cdot (A \vec a + A \vec b)}\\
                                          &= \sqrt{A \vec a \cdot A \vec a + 2A \vec a \cdot A \vec b + A \vec b \cdot A \vec b}\\
                                          &= \sqrt{\lVert A \vec a \rVert^2 + 2A \vec a \cdot A \vec b + \lVert A \vec b \rVert^2}\\
                                          &= \sqrt{\lVert \vec a \rVert^2 + 2A \vec a \cdot A \vec b + \lVert \vec b \rVert^2}
    \end{align*}
    이고
    \begin{align*}
        \lVert \vec a + \vec b \rVert &= \sqrt{(\vec a + \vec b \cdot \vec a + \vec b)}\\
                                      &= \sqrt{\vec a \cdot \vec a + 2 \vec a \cdot \vec b + \vec b \cdot \vec b}\\
                                      &= \sqrt{\lVert \vec a \rVert^2 + 2 \vec a \cdot \vec b + \lVert \vec b \rVert^2}
    \end{align*}
    이므로,
    \begin{equation*}
        A \vec a \cdot A \vec b = \vec a \cdot \vec b
    \end{equation*}
    를 만족한다.
    따라서 3이면 4가 성립하며, 3과 4는 동치이다.

    마지막으로 4와 6이 동치임을 보이자. 먼저 4이면 6임을 보이자.

    $A$의 열벡터들 $\vec a_1, \dots, \vec a_n$은 각각 $A \vec e_1, \dots, A \vec e_n$와 같다.
    따라서 각 열벡터들이 정규직교한다는 것은
    \begin{equation*}
        \lVert A \vec e_i \rVert = 1
    \end{equation*}
    이고
    \begin{equation*}
        A \vec e_i \cdot A \vec e_j = \delta_{ij}
    \end{equation*}
    라는 것이다.
    그런데 $\vec e_i \cdot \vec e_j = \delta_{ij}$이며 4에 의해 $A \vec e_i \cdot A \vec e_j = \vec e_i \cdot \vec e_j$이므로, $A \vec e_i \cdot A \vec e_j = \delta_{ij}$이다.
    나아가 4와 동치인 3에 의해 $\lVert A \vec e_i\rVert = \lVert A \vec e_i \rVert = 1$이다.
    따라서 4이면 6이다.

    이제 6이면 4를 보이자.
    벡터 $\vec a, \vec b \in \mathbb R^n$에 대해서,
    \begin{align*}
        \vec a &= (a_1, \dots, a_n)\\
        \vec b &= (b_1, \dots, b_n)
    \end{align*}
    라고 하면
    \begin{align*}
        A \vec a \cdot A \vec b &= A(a_1 \vec e_1 + \dots + a_n \vec e_n) \cdot A (b_1 \vec e_1 + \dots + b_n \vec e_n)\\
                                &= \sum_{i, j = 1}^n A a_i \vec e_i \cdot A b_j \vec e_j = \sum_{i, j = 1}^n a_i b_j A \vec e_i \cdot A \vec e_j\\
                                &= \sum_{i, j = 1}^n a_i b_j \delta_{ij} = \sum_{i = 1}^n a_i b_i = \vec a \cdot \vec b
    \end{align*}
    이므로 4가 성립한다.
    따라서 4와 6이 동치이며, 1, 2, 3, 4, 5, 6 모두가 동치인 명제임을 알 수 있다.
\end{proof}

\begin{definition} \label{def:orthogonal_mat}
    $n \times n$ 행렬 $A$에 대해서 정리~\ref{thm:orthogonal_mat}의 조건들을 만족하는 행렬을 직교(orthogonal)행렬이라고 한다.
\end{definition}

\begin{theorem}
    선형 등거리 변환(linear isometry) $L_A: \mathbb R^n \rightarrow \mathbb R^n$에 대응되는 행렬 $A$는 직교행렬이다.
\end{theorem}

\begin{example}
    \leavevmode
    \begin{enumerate}
        \item $\mathbb R^2 \rightarrow \mathbb R^2$에서 원점을 기준으로 $\theta$만큼 회전시키는 회전변환 
            \begin{equation*}
                \vec x \mapsto \begin{pmatrix}\cos \theta & -\sin \theta \\ \sin \theta & \cos \theta\end{pmatrix} \vec x
            \end{equation*}
        의 행렬은 직교행렬이다.
        \item $\mathbb R^2 \rightarrow \mathbb R^2$에서 원점을 지나고 $x$축과 $\theta$의 각을 이루는 직선에 대한 대칭변환 
            \begin{equation*}
                \vec x \mapsto \begin{pmatrix}\cos \theta & \sin \theta \\ \sin \theta & -\cos \theta\end{pmatrix} \vec x
            \end{equation*}
        의 행렬은 직교행렬이다.
    \end{enumerate}
\end{example}

\begin{remark}
    $\mathbb R^3 \rightarrow \mathbb R^3$에서\marginpar{\small 2018.10.8.} 벡터 $\vec v$를 기준으로 $\alpha$만큼 회전시키는 회전변환 $R_{\vec v}(\alpha)$를 구하자.

    우선 $z$ 축을 중심으로 $\alpha$만큼 회전하는 $R_{\vec e_3}(\alpha)$는 $\mathbb R^2$에서의 회전변환을 통해
    \begin{equation*}
        R_{\vec e_3}(\alpha)
        \begin{pmatrix}
            x \\ y \\ z
        \end{pmatrix}
        =
        \begin{pmatrix}
            \cos \alpha & -\sin \alpha & 0\\
            \sin \alpha & \cos \alpha & 0\\
            0 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
            x \\ y \\ z
        \end{pmatrix}
    \end{equation*}
    임을 쉽게 알 수 있다.

    이제 $\vec v$와 직교를 이루는 $\vec u, \vec w$를 잡자.
    이때 $\vec w \times \vec u$가 $\vec v$와 평행이 되도록 하자.
    이들을 정규화한 $\sfrac{\vec w}{\lVert \vec w \rVert}, \sfrac{\vec u}{\lVert \vec u \rVert}, \sfrac{\vec v}{\lVert \vec v \rVert}$를 각각 $\vec e_1, \vec e_2, \vec e_3$로 변환하는 선형변환 $L$을 잡자.
    즉,
    \begin{align*}
        L\left(\frac{\vec w}{\lVert \vec w \rVert}\right) &= \vec e_1\\
        L\left(\frac{\vec u}{\lVert \vec u \rVert}\right) &= \vec e_2\\
        L\left(\frac{\vec v}{\lVert \vec v \rVert}\right) &= \vec e_3
    \end{align*}
    이다.
    그런데 $L$은 회전변환이므로 반드시 역함수 $L^{-1}$가 존재한다.
    따라서
    \begin{align*}
        \frac{\vec w}{\lVert \vec w \rVert} &= L^{-1}(\vec e_1)\\
        \frac{\vec u}{\lVert \vec u \rVert} &= L^{-1}(\vec e_2)\\
        \frac{\vec v}{\lVert \vec v \rVert} &= L^{-1}(\vec e_3)
    \end{align*}
    이고, $L$에 대응되는 행렬은 
    \begin{equation*}
        \begin{pmatrix}
            \frac{\vec w}{\lVert \vec w \rVert} & \frac{\vec u}{\lVert \vec u \rVert} & \frac{\vec v}{\lVert \vec v \rVert}
        \end{pmatrix}
    \end{equation*}
    이다.
    그런데 $\sfrac{\vec w}{\lVert \vec w \rVert}, \sfrac{\vec u}{\lVert \vec u \rVert}, \sfrac{\vec v}{\lVert \vec v \rVert}$는 정규직교하므로 이는 직교행렬이고, 따라서 역행렬은
    \begin{equation*}
        \begin{pmatrix}
            \frac{\vec w^t}{\lVert \vec w \rVert} \\ \frac{\vec u^t}{\lVert \vec u \rVert} \\ \frac{\vec v^t}{\lVert \vec v \rVert}
        \end{pmatrix}
    \end{equation*}
    이다.

    $\vec v$를 기준으로 $\alpha$만큼의 회전은 $\sfrac{\vec v}{\lVert \vec v\rVert}$가 $\vec e_3$가 되도록 회전한 후, $z$ 축을 중심으로 $\alpha$만큼의 회전, 즉 $R_{\vec e_3}(\alpha)$를 한 후, 다시 원래대로 $\vec e_3$를 $\sfrac{\vec v}{\lVert \vec v\rVert}$의 위치로 옮기는 것이다.
    따라서 $R_{\vec v}(\alpha)$는 아래와 같은 행렬에 대응된다:
    \begin{equation*}
        \begin{pmatrix}
            \frac{\vec w}{\lVert \vec w \rVert} & \frac{\vec u}{\lVert \vec u \rVert} & \frac{\vec v}{\lVert \vec v \rVert}
        \end{pmatrix}
        \begin{pmatrix}
            \cos \alpha & -\sin \alpha & 0\\
            \sin \alpha & \cos \alpha & 0\\
            0 & 0 & 1
        \end{pmatrix}
        \begin{pmatrix}
            \frac{\vec w^t}{\lVert \vec w \rVert} \\ \frac{\vec u^t}{\lVert \vec u \rVert} \\ \frac{\vec v^t}{\lVert \vec v \rVert}
        \end{pmatrix}
    \end{equation*}
\end{remark}

벡터함수 $\vec F = (f_1, \dots, f_n)$가 $\mathbb R^m \rightarrow \mathbb R^n$으로 갈 때, 점 $P$에서 다음과 같이 근사할 수 있다:
\begin{align*}
    \vec F(\vec x) &=
    \begin{pmatrix}
        f_1(\vec x) \\ \vdots \\ f_n(\vec x)
    \end{pmatrix}
    \approx
    \begin{pmatrix}
        f_1(P) + f_1'(P) (\vec x - P)\\
        \vdots\\
        f_n(P) + f_n'(P) (\vec x - P)
    \end{pmatrix}\\
              &=
              \begin{pmatrix}
                  f_1'(P) \\ \vdots \\ f_n'(P)
              \end{pmatrix}
              (\vec x - P) + \vec F(P)
\end{align*}
이때 $\vec F$의 $P$에서의 야코비 행렬(Jacobian matrix)을 다음과 같이 정의하여 더 간단하게 표현할 수 있다.

\begin{definition}
    벡터함수 $\vec F: \mathbb R^m \rightarrow \mathbb R^n$에 대해 야코비 행렬(Jacobian matrix)을 다음과 같이 정의한다:
    \begin{equation*}
        \vec F' = \jacob{f_1, \dots, f_n}{x_1, \dots, x_m} =
        \begin{pmatrix}
            f_1' \\ \vdots \\ f_n'
        \end{pmatrix}
    \end{equation*}
    이때 $\vec F = (f_1, \dots, f_n)$이다.
\end{definition}

따라서 $\vec F$의 $P$에서의 근사는
\begin{equation*}
    \vec F(\vec x) \approx \vec F'(P) (\vec x - P) + \vec F(P)
\end{equation*}
로 쓸 수 있다.

\begin{example}
    각도를 보존하는 사상을 등각사상(conformal mapping)이라고 하는데, 어떤 사상의 야코비 행렬이 직교행렬의 상수 배이면 해당 사상은 등각사상임이 알려져 있다.
    $\vec F(x, y) = (f(x, y), g(x, y)) = (x^2 - y^2, -2xy)$로 정의된 벡터함수의 야코비 행렬을 구하면
    \begin{equation*}
        \vec F' =
        \begin{pmatrix}
            \diffp{f}{x} & \diffp{f}{y}\\
            \diffp{g}{x} & \diffp{g}{y}
        \end{pmatrix}
        =
        \begin{pmatrix}
            2x & -2y\\
            -2y & -2x
        \end{pmatrix}
    \end{equation*}
    이다.
    그런데 $\vec F'$의 열벡터 $2(x, -y)$와 $2(-y, -x)$의 내적이 0이므로 두 열벡터는 직교한다.
    따라서 $\vec F'$는 직교행렬의 상수배임을 알 수 있고, $\vec F$는 등각사상이다.
\end{example}

\begin{definition}
    $n \times n$ 행렬 $A$에 대해서 $A \vec x = \lambda \vec x$가 되는 $\vec x \neq \vecz$가 존재한다.
    이때 $\lambda$를 특성값(eigenvalue), $\vec x$를 $\lambda$-특성벡터(eigenvector) 혹은 단순히 특성벡터라고 부르며, $\lambda$-특성벡터들과 $\vecz$의 집합 $E_\lambda$를 $\lambda$-특성공간(eigenspace)이라고 부른다.
    또한 $\det (A - \lambda I)$을 특성다항식(characteristic polynomial)이라고 부르고, $\Char A$라고 쓴다.
\end{definition}

\begin{remark}
    $n \times n$ 행렬 $A$의 특성값 $\lambda$와 특성벡터 $\vec x$를 구하기 위해서,
    \begin{equation*}
        A \vec x = \lambda \vec x
    \end{equation*}
    에서 우변을 좌변으로 이항한다:
    \begin{equation*}
        (A - \lambda I) \vec x = \vecz
    \end{equation*}
    그런데 $\vec x \neq \vecz$이므로 $A - \lambda I$는 정리~\ref{thm:one_to_one_nullspace}와 \ref{thm:nonzero_det}에 의해 $\det (A - \lambda I) = 0$이어야 한다.
    따라서 특성다항식 $\Char A = \det (A - \lambda I) = 0$을 풀면 특성값 $\lambda$를 구할 수 있다.
    $\lambda$-특성벡터 $\vec x$의 경우, $(A - \lambda I) \vec x = \vecz$이므로 $\Null (A - \lambda I)$의 원소를 구하면 된다.
    이는 $A - \lambda I$를 행사다리꼴 형태로 바꾸어 구할 수 있다.
\end{remark}

\begin{example}
    \leavevmode
    \begin{enumerate}
        \item 행렬 $\begin{pmatrix}2 & 1\\1 & 2\end{pmatrix}$의 특성값들과 각각의 특성벡터들을 찾기 위해 특성다항식을 푼다:
            \begin{equation*}
                \det
                \begin{pmatrix}
                    2 - \lambda & 1\\
                    1 & 2 - \lambda
                \end{pmatrix}
                = (2 - \lambda)^2 - 1 = (\lambda - 3)(\lambda - 1) = 0
            \end{equation*}
            따라서 $\lambda = 3$일 경우와 $\lambda = 1$일 경우를 나누어 특성벡터를 찾는다.
            \begin{enumerate}
                \item $\lambda = 1$일 때
                    \begin{equation*}
                        \begin{pmatrix}
                            1 & 1\\
                            1 & 1
                        \end{pmatrix}
                        \vec x = \vecz
                    \end{equation*}
                    이므로 $k \in \mathbb R$에 대해 1-특성벡터 $\vec x = k(1, -1)$이며 1-특성공간 $E_1 = \langle 1, -1 \rangle$이다.
                \item $\lambda = 3$일 때
                    \begin{equation*}
                        \begin{pmatrix}
                            -1 & 1\\
                            1 & -1
                        \end{pmatrix}
                        \vec x = \vecz
                    \end{equation*}
                    이므로 $k \in \mathbb R$에 대해 3-특성벡터 $\vec x = k(1, 1)$이며 3-특성공간 $E_3 = \langle 1, 1 \rangle$이다.
            \end{enumerate}
        \item $0 < \alpha < \pi$에 대해 행렬 $A = \begin{pmatrix}\cos \alpha & -\sin \alpha\\\sin \alpha & \cos \alpha\end{pmatrix}$의 특성값은 실수 범위에서 존재하지 않는다.
            이는
            \begin{equation*}
                \Char A = \lambda^2 - 2 \lambda \cos \alpha + 1
            \end{equation*}
            인데, $\lambda$에 대한 이차방정식의 판별식에서 $\sfrac D4 = \cos^2 \alpha - 1 < 0$이기 때문에 실근을 가지지 않는다.
            하지만 복소수 범위에서 $\lambda = e^{i \alpha}$ 혹은 $\lambda = e^{-i \alpha}$임을 알 수 있다.
        \item 행렬\marginpar{\small 2018.10.10.}  $\begin{pmatrix}-2 & 2 & -3\\2 & 1 & -6\\-1 & -2 & 0\end{pmatrix}$의 특성값들과 각각의 특성벡터들을 찾기 위해 특성다항식을 푼다:
            \begin{equation*}
                \det
                \begin{pmatrix}
                    -2 - \lambda & 2 & -3\\
                    2 & 1 - \lambda & -6\\
                    -1 & -2 & -\lambda
                \end{pmatrix}
                = - (\lambda - 5)(\lambda + 3)^2
            \end{equation*}
            따라서 $\lambda = 5$일 경우와 $\lambda = -3$일 경우를 나누어 특성벡터를 찾는다.
            \begin{enumerate}
                \item $\lambda = 5$일 때
                    \begin{equation*}
                        \begin{pmatrix}
                            -7 & 2 & -3\\
                            2 & -4 & -6\\
                            -1 & -2 & -5
                        \end{pmatrix}
                        \rightarrow
                        \begin{pmatrix}
                            -7 & 2 & -3\\
                            0 & 1 & 2\\
                            0 & 0 & 0
                        \end{pmatrix}
                        \rightarrow
                        \begin{pmatrix}
                            1 & 0 & 1\\
                            0 & 1 & 2\\
                            0 & 0 & 0
                        \end{pmatrix}
                    \end{equation*}
                    이므로 $k \in \mathbb R$에 대해 5-특성벡터 $\vec x = k(-1, -2, 1)$이며 5-특성공간 $E_5 = \langle (-1, -2, 1) \rangle$이다.
                \item $\lambda = -3$일 때
                    \begin{equation*}
                        \begin{pmatrix}
                            1 & 2 & -3\\
                            2 & 4 & -6\\
                            -1 & -2 & 3
                        \end{pmatrix}
                        \rightarrow
                        \begin{pmatrix}
                            1 & 2 & -3\\
                            0 & 0 & 0\\
                            0 & 0 & 0
                        \end{pmatrix}
                    \end{equation*}
                    이므로 $k, l \in \mathbb R$에 대해 $-3$-특성벡터 $\vec x = k(-2, 1, 0) + l(3, 0, 1)$이며 $-3$-특성공간 $E_{-3} = \langle (-2, 1, 0), (3, 0, 1)\rangle$이다.
            \end{enumerate}
            여기서 $\lambda = 5$일 때 $\Char A$에서는 $\lambda - 5$ 항이 하나이고 $\dim E_5 = 1$로 같고, $\lambda = -3$일 때 $\Char A$에서 $\lambda + 3$ 항이 두 개, $\dim E_{-3} = 2$로 동일하다.
            그러나 이렇게 특성방정식에서 중근의 개수와 해당 특성값에 대응되는 특성공간의 차원이 항상 같은 것은 아니다.
    \end{enumerate}
\end{example}

\begin{definition}
    행렬 $A$에 대해서, $\Char A$의 어떤 해, 즉 특성값이 나타나는 횟수를 대수적 중복도(algebraic multiplicity), 해당 특성값에 대응되는 특성공간의 차원을 기하적 중복도(geometric multiplicity)라고 한다.
\end{definition}

\begin{theorem}
    행렬 $A$의 기하적 중복도는 대수적 중복도를 초과하지 못한다.
\end{theorem}

\begin{example}
    $\begin{pmatrix}1 & 0\\0 & 1\end{pmatrix}$의 특성방정식은 $(\lambda - 1)^2 = 0$이다.
    따라서 $\lambda = 1$의 대수적 중복도는 1이고, $A - \lambda I = O$이므로 $\mathbb R^2$의 임의의 벡터가 특성벡터이다.
    그러므로 특성공간은 $\mathbb R^2$이고, 기하적 중복도는 2이다.

    $\begin{pmatrix}0 & 1\\0 & 0\end{pmatrix}$의 특성방정식은 $\lambda^2 = 0$이다.
    따라서 $\lambda = 0$의 대수적 중복도는 2이고, $k \in \mathbb R$에 대해 $(k, 0)$이 특성벡터이므로, 특성공간은 $\langle (1, 0) \rangle$로 기하적 중복도가 1이다.

    위의 두 경우 모두 기하적 중복도가 대수적 중복도 이하임을 확인할 수 있다.
\end{example}

\begin{definition}
    벡터공간 $V$와 부분공간 $U, W$에 대해서 $U \cap W = \{\vecz\}$일 때 $U$와 $W$의 합을 직합(direct sum)이라고 하고 $U \oplus W$라고 쓴다.
\end{definition}

\begin{remark}
    그라스만 공식(정리~\ref{thm:grassmann})에서 $\dim (U + W) = \dim U + \dim W$임을 알 수 있다.
\end{remark}

\begin{theorem}
    두 벡터공간 $U$와 $W$의 교집합이 $\{\vecz\}$일 때, $\vec x \in U \oplus W$에 대해서
    \begin{equation*}
        \vec x = \vec u_1 + \vec w_1 = \vec u_2 + \vec w_2
    \end{equation*}
    이고 $\vec u_1, \vec u_2 \in U$, $\vec w_1, \vec w_2 \in W$일 때, $\vec u_1 = \vec u_2$이고 $\vec w_1 = \vec w_2$이다.
    즉, $U \oplus W$의 원소의 합 표현은 유일하다.
\end{theorem}

\begin{proof}
    $\vec x \in U \oplus W$에 대해서 $\vec u_1 + \vec w_1 = \vec u_2 + \vec w_2 = \vec x$이고 $\vec u_1, \vec u_2 \in U$, $\vec w_1, \vec w_2 \in W$라고 하자.
    그러면 $\vec u_1 - \vec u_2 = \vec w_2 - \vec w_1$이므로 $U \cap W = \{\vecz\}$의 원소이다.
    따라서 $\vec u_1 - \vec u_2 = \vec w_2 - \vec w_1 = \vecz$이어서 $\vec x$의 합 표현은 유일하다.
\end{proof}

\begin{definition}
    벡터공간 $V$의 부분공간 $U_1, \dots, U_k$에 대해서 어떤 벡터 $\vec x \in U_1 + \dots + U_k$의 합 표현이 유일할 때
    \begin{equation*}
        U_1 + \dots + U_k = U_1 \oplus \dots \oplus U_k
    \end{equation*}
    로 쓴다.
\end{definition}

\begin{theorem} \label{thm:linear_independence_directsum}
    벡터공간 $U_1, \dots, U_k$에 대해서 $U_1 \oplus \dots \oplus U_k$가 존재할 때,
    \begin{equation*}
        \bigl\{\vec u_i \bigm| \vec u_i \in U_i \setminus \{\vecz\},\ i \in \{1, \dots, k\}\bigr\}
    \end{equation*}
    은 일차독립이다.
\end{theorem}

\begin{proof}
    스칼라 $c_1, \dots, c_k$에 대해서
    \begin{equation*}
        c_1 \vec u_1 + \dots + c_k \vec u_k = \vecz
    \end{equation*}
    라고 가정하자.
    $\vec x$를
    \begin{equation*}
        \vec x = c_1 \vec u_1 + \dots + c_{k - 1} \vec u_{k - 1} = -c_k \vec u_k
    \end{equation*}
    로 두면,
    \begin{align*}
        \vec x &= c_1 \vec u_1 + \dots + c_{k - 1} \vec u_{k - 1} + 0 \vec u_k\\
               &= 0 \vec u_1 + \dots + 0 \vec u_{k - 1} + (-c_k) \vec u_k
    \end{align*}
    이고 합 표현의 유일성에 따라 $c_1 = \dots = c_k = 0$이다.
    그러므로 $\bigl\{\vec u_i \bigm| \vec u_i \in U_i \setminus \{\vecz\},\ i \in \{1, \dots, k\}\bigr\}$은 일차독립이다.
\end{proof}

\begin{theorem} \label{thm:eigenspace_directsum}
    $n \times n$ 행렬 $A$에 대해서 서로 다른 특성값 $\lambda_1, \dots, \lambda_k$에 대응되는 특성공간 $E_1, \dots, E_n$에 대해, 다음이 성립한다:
    \begin{equation*}
        E_1 + \dots + E_k = E_1 \oplus \dots \oplus E_k
    \end{equation*}
\end{theorem}

\begin{proof}
    $k$에 대한 수학적 귀납법으로 증명한다.

    먼저 $k = 2$일 경우, 어떤 벡터 $\vec v \in E_1 \cap E_2$를 고르자.
    그러면
    \begin{equation*}
        A \vec v = \lambda_1 \vec v = \lambda_2 \vec v
    \end{equation*}
    이므로,
    \begin{equation*}
        (\lambda_1 - \lambda_2) \vec v = \vecz
    \end{equation*}
    이다.
    그런데 $\lambda_1 \neq \lambda_2$이므로 $\vec v = \vecz$이다.
    따라서 $E_1 + E_2 = E_1 \oplus E_2$이다.

    이제 $k > 2$의 경우에, $E_1 + \dots + E_{k - 1} = E_1 \oplus \dots \oplus E_{k - 1}$가 성립한다고 가정하자.
    $\vec x \in E_1 + \dots + E_k$에 대해서 $\vec v_i, \vec u_i \in E_i$일 때
    \begin{equation*}
        \vec x = \vec v_1 + \dots + \vec v_k = \vec u_1 + \dots + \vec u_k \in E_1 + \dots + E_k
    \end{equation*}
    라고 하자.
    그러면
    \begin{equation*}
        (\vec v_1 - \vec u_1) + \dots + (\vec v_{k - 1} - \vec u_{k - 1}) = \vec u_k - \vec v_k
    \end{equation*}
    이고, $\vec v_i - \vec u_i = \vec w_i$로 놓자.
    따라서
    \begin{equation*}
        \vec w_1 + \dots + \vec w_{k - 1} = -\vec w_k
    \end{equation*}
    이고 $\vec w_i \in E_i$이다.
    양변에 $A$를 왼쪽에 곱하면
    \begin{align*}
        &A(\vec w_1 + \dots + \vec w_{k - 1}) = A(-\vec w_k)\\
        \Leftrightarrow\quad &A \vec w_1 + \dots + A \vec w_{k - 1} = -A \vec w_k\\
        \Leftrightarrow\quad &\lambda_1 \vec w_1 + \dots + \lambda_{k - 1} \vec w_{k - 1} = - \lambda_k \vec w_k
    \end{align*}
    이다.
    또한 양변에 $\lambda_k$를 곱한 것은
    \begin{equation*}
        \lambda_k \vec w_1 + \dots + \lambda_k \vec w_{k - 1} = -\lambda_k \vec w_k
    \end{equation*}
    이다.
    둘을 양변에서 빼면
    \begin{equation*}
        (\lambda_1 - \lambda_k) \vec w_1 + \dots + (\lambda_{k - 1} - \lambda_k) \vec w_{k - 1} = \vecz
    \end{equation*}
    이다.
    따라서 $i \in \{1, \dots, k - 1\}$에 대해 $\lambda_i \neq \lambda_k$이므로 $\vec w_1, \dots, \vec w_{k - 1}$는 일차종속이다.
    그런데 귀납 가정에 따라 $E_1 + \dots + E_{k - 1} = E_1 \oplus \dots \oplus E_{k - 1}$이며 정리~\ref{thm:linear_independence_directsum}에 따라 $\vecz$가 아닌 벡터들은 일차독립이어야 하므로, $\vec w_1, \dots, \vec w_{k - 1}$은 $\vecz$이다.
    그러므로 $\vec v_i = \vec u_i$이고, 벡터 $\vec x \in E_1 + \dots + E_k$의 합 표현은 유일하고, $E_1 + \dots + E_k = E_1 \oplus \dots \oplus E_k$이다.
    이로써 귀납 증명이 완료된다.
\end{proof}

\begin{remark}
    벡터공간 $U_1, \dots, U_k$에 대해서
    \begin{equation*}
        U_1 + \dots + U_k = U_1 \oplus \dots \oplus U_k \Rightarrow \bigl(\forall i, j \in \{1, \dots, k\}\quad i \neq j \Rightarrow U_i \cap U_j = \{\vecz\}\bigr)
    \end{equation*}
    이지만 그 역은 성립하지 않는다.
    예를 들어, $\mathbb R^2$ 좌표평면에서 원점을 지나는 세 직선으로 나타내어지는 벡터공간의 교집합은 원점뿐이지만, $\mathbb R^2$의 벡터들은 세 직선에 속하는 벡터들의 합 표현으로 유일하게 나타내지지 않는다.

    그러나 벡터공간 $U_1, \dots, U_k$들의 집합 $\mathcal U$에 대해서,
    \begin{equation*}
        \forall \mathcal P_1, \mathcal P_2 \subseteq \mathcal U \qquad \mathcal P_1 \cap \mathcal P_2 = \varnothing \Rightarrow \sum_{U \in \mathcal P_1} U\ \cap \sum_{\tilde U \in \mathcal P_2} \tilde U = \{\vecz\}
    \end{equation*}
    인 것은 $U_1 + \dots + U_k = U_1 \oplus \dots \oplus U_k$와 동치이다.
\end{remark}

\begin{theorem}
    행렬 $A$의 서로 다른 특성공간 $E_1, \dots E_k$와 각각의 기저가 $\mathcal B_1, \dots, \mathcal B_k$이라고 하자.
    그러면 $\mathcal B_1 \cup \dots \cup \mathcal B_k$는 $E_1 \oplus \dots \oplus E_k$의 기저이다.
\end{theorem}

\begin{proof}
    $i \in \{1, \dots, k\}$에 대해 $E_i$의 기저 $\mathcal B_i = \{\vec v_{i1}, \dots, \vec v_{ij_i}\}$라고 하자.
    먼저 $\mathcal B_1 \cup \dots \cup \mathcal B_k$가 $E_1 \oplus \dots \oplus E_k$를 생성하는 것은 각각의 $\mathcal B_i$가 $E_i$를 생성하므로 당연하다.
    이제 $\mathcal B_1 \cup \dots \cup \mathcal B_k$가 일차독립임을 보이자.

    기저 $\mathcal B_i$에 속하는 기저 벡터들의 선형결합들의 합을
    \begin{equation*}
        \sum_{i = 1}^k (c_{i1} \vec v_{i1} + \dots + c_{ij_i} \vec v_{ij_i}) = \vecz
    \end{equation*}
    라고 하자.
    $l \in \{1, \dots, k\}$를 하나 골라서 우변으로 이항하면,
    \begin{equation*}
        \sum_{i \neq l} (c_{i1} \vec v_{i1} + \dots + c_{ij_i} \vec v_{ij_i}) = -(c_{l1} \vec v_{l1} + \dots + c_{lj_l} \vec v_{lj_l})
    \end{equation*}
    이다.
    그런데 정리~\ref{thm:eigenspace_directsum}에 따라 양변은 $E_1 \oplus \dots \oplus E_k$의 원소이므로 합 표현이 유일하고, 따라서 모든 $c_{ij_i}$는 0이다.
    따라서 $\mathcal B_1 \cup \dots \cup \mathcal B_k$가 일차독립이다.
\end{proof}

\begin{definition}
    $L_A: \mathbb R^n \rightarrow \mathbb R^n$에 대응되는 $n \times n$ 행렬 $A$가 있을 때, $\vec v_1, \dots, \vec v_n$이 $\mathbb R^n$에 대한 행렬 $A$의 특성기저(eigenbasis)라는 것은 $\vec v_1, \dots, \vec v_n$이 $A$의 특성벡터이면서 $\mathbb R^n$의 기저벡터라는 것을 말한다.
\end{definition}

\begin{remark}
    $\mathbb R^n$에 대한 $n \times n$ 행렬 $A$의 특성기저가 존재한다는 것은 $\Char A$의 중근을 포함한 실근의 개수가 $n$개라는 것이고, 이는 대수적 중복도의 합과 기하적 중복도의 합이 $n$으로 같다는 것이다.
\end{remark}

\begin{definition}
    $n \times n$\marginpar{2018.10.15} 행렬 $A$와 $B$에 대해서 $A$와 $B$가 닮았다(similar)는 것은 $n \times n$ 가역행렬 $P$가 존재해서
    \begin{equation*}
        A = P^{-1} B P
    \end{equation*}
    라는 것이다.
    이 때 $A \sim B$라고 표기한다.
\end{definition}

\begin{theorem}
    $n \times n$ 행렬 $A$와 $B$에 대해서 다음이 성립한다:
    \begin{enumerate}
        \item $A \sim A$ (반사성(reflexivity))
        \item $A \sim B \Rightarrow B \sim A$ (대칭성(symmetricity))
        \item $A \sim B \,\wedge\, B \sim A \Rightarrow A \sim C$ (추이성(transivity))
    \end{enumerate}
\end{theorem}

\begin{proof}
    \leavevmode
    \begin{enumerate}
        \item 반사성은 자명하게 성립한다.
        \item 대칭성은 $A = P^{-1} B P$이면 $B = (P^{-1})^{-1} A (P^{-1})$이므로 성립한다.
        \item 추이성은 $A = P^{-1}BP$이고 $B = Q^{-1} C Q$일 때 $A = (QP)^{-1} C (QP)$이므로 성립한다.
    \end{enumerate}
\end{proof}

\begin{definition}
    $n \times n$ 행렬의 주대각선을 제외한 곳의 원소가 모두 0이면 대각행렬(diagonal matrix)이라고 부른다.
    또한 대각행렬과 닮은 행렬을 대각화 가능 행렬(diagonalizable matrix)이라고 부른다.
\end{definition}

\begin{theorem}
    $n \times n$ 행렬 $A$가 대각화 가능 행렬일 필요충분조건은 $A$이 $\mathbb R^n$에 대해 특성기저를 가지는 것이다.
\end{theorem}

\begin{proof}
    먼저 $\mathbb R^n$에 대한 $n \times n$ 행렬 $A$의 특성기저가 존재하면 $A$가 대각화 가능하다는 것을 보이자.

    $\mathbb R^n$에 대한 $n \times n$ 행렬 $A$의 특성기저를 $\{\vec v_1, \dots, \vec v_n\}$, 각각에 대응되는 특성값을 $\lambda_1, \dots, \lambda_n$이라고 하자.
    즉,
    \begin{align*}
        A\vec v_1 &= \lambda_1 \vec v_1\\
                  &\vdots\\
        A\vec v_n &= \lambda_n \vec v_n
    \end{align*}
    이며, 이는 다시
    \begin{equation*}
        A
        \begin{pmatrix}
            \vec v_1 & \dots & \vec v_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            \vec v_1 & \dots & \vec v_n
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1 & \dots & 0\\
            \vdots & \ddots & \vdots\\
            0 & \dots & \lambda_n
        \end{pmatrix}
    \end{equation*}
    로 쓸 수 있다.
    따라서
    \begin{equation*}
        A
        =
        \begin{pmatrix}
            \vec v_1 & \dots & \vec v_n
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1 & \dots & 0\\
            \vdots & \ddots & \vdots\\
            0 & \dots & \lambda_n
        \end{pmatrix}
        \begin{pmatrix}
            \vec v_1^t \\ \vdots \\ \vec v_n^t
        \end{pmatrix}
    \end{equation*}
    이므로 $A$는 대각화 가능 행렬이다.

    이제 $A$가 대각화 가능하면 $\mathbb R^n$에 대해 $n \times n$ 행렬 $A$의 특성기저가 존재한다는 것을 보이자.

    $n \times n$ 가역행렬 $P$과 대각행렬 $D$에 대해서 $A = PDP^{-1}$이라고 하자.
    $P$의 열벡터들을 $\vec v_1, \dots, \vec v_n$으로 놓고, $i \in \{1, \dots, n\}$일 때 $D$의 $i$행 $i$열의 원소를 $\lambda_i$로 놓자.
    $\lambda_i$들이 $A$의 특성값이고, $\vec v_i$가 $\lambda_i$에 대응되는 특성벡터임을 보이자.
    \begin{equation*}
        A
        \begin{pmatrix}
            \vec v_1 & \dots & \vec v_n
        \end{pmatrix}
        =
        \begin{pmatrix}
            \vec v_1 & \dots & \vec v_n
        \end{pmatrix}
        \begin{pmatrix}
            \lambda_1 & \dots & 0\\
            \vdots & \ddots & \vdots\\
            0 & \dots & \lambda_n
        \end{pmatrix}
    \end{equation*}
    이므로 모든 $\vec v_i$에 대해 $A \vec v_i = \lambda_i \vec v_i$임을 알 수 있다.
    따라서 $\lambda_i$들은 $A$의 특성값이고 $\vec v_i$는 $\lambda_i$-특성벡터이다.
    이제 $\vec v_i$들이 $\mathbb R^n$의 기저벡터임을 보이자.
    $P$는 가역행렬이므로 열벡터들이 일차독립이다(쪽~\pageref{page:equiv_remark_update}의 7과 9).
    나아가 정리~\ref{thm:basis_span}에 따라 $\vec v_i$들은 기저를 이룬다.
\end{proof}

\begin{remark}
    위의 증명 과정에서 임의의 특성기저를 상수배하여도 대각화할 때 사용되는 가역행렬의 열벡터로 사용할 수 있다는 것을 알 수 있다.
    또한, 두 열벡터를 교환하면 각각의 특성벡터에 해당하는 특성값을 교환하면 여전히 대각화가 성립한다.
    즉,
    \begin{align*}
        A &=
        \begin{pmatrix}
            1 & 4 & 7\\
            2 & 5 & 8\\
            3 & 6 & 10
        \end{pmatrix}
        \begin{pmatrix}
            4 & 0 & 0\\
            0 & 5 & 0\\
            0 & 0 & 6
        \end{pmatrix}
        \begin{pmatrix}
            1 & 4 & 7\\
            2 & 5 & 8\\
            3 & 6 & 10
        \end{pmatrix}^t\\
          &=
        \begin{pmatrix}
            2 & 4 & 7\\
            4 & 5 & 8\\
            6 & 6 & 10
        \end{pmatrix}
        \begin{pmatrix}
            4 & 0 & 0\\
            0 & 5 & 0\\
            0 & 0 & 6
        \end{pmatrix}
        \begin{pmatrix}
            2 & 4 & 7\\
            4 & 5 & 8\\
            6 & 6 & 10
        \end{pmatrix}^t\\
          &=
        \begin{pmatrix}
            4 & 2 & 7\\
            5 & 4 & 8\\
            6 & 6 & 10
        \end{pmatrix}
        \begin{pmatrix}
            5 & 0 & 0\\
            0 & 4 & 0\\
            0 & 0 & 6
        \end{pmatrix}
        \begin{pmatrix}
            4 & 2 & 7\\
            5 & 4 & 8\\
            6 & 6 & 10
        \end{pmatrix}^t
    \end{align*}
    이다.
\end{remark}

\begin{theorem}
    $n \times n$ 행렬 $A$가 대각화 가능하면, 즉 어떤 $n \times n$ 가역행렬 $P$와 대각행렬 $D$에 대해서 $A = PDP^{-1}$일 때,
    \begin{equation*}
        A^k = PD^k P^{-1}
    \end{equation*}
    이다.
\end{theorem}

\begin{proof}
    $A = PDP^{-1}$일 때
    \begin{equation*}
        A^k = (PDP^{-1})^k = \underbrace{(PDP^{-1}) \dots (PDP^{-1})}_\text{$k$ 개} = P\underbrace{D\dots D}_\text{$k$ 개}P^{-1} = PD^kP^{-1}
    \end{equation*}
    이다.
\end{proof}

\begin{theorem}
    대칭 실행렬 $A$는 대각화 가능 행렬이다.
    즉, $A = A^t$인 것은 $\Char A$의 모든 근이 실수이고 모든 근에 대해 대수적 중복도와 기하적 중복도가 같은 것과 동치이다.
\end{theorem}

\begin{lemma} \label{lem:self_adjoint_sym}
    $n \times n$ 행렬 $A$가 대칭행렬인 것은 $A$가 스칼라곱에 대해 자기 수반(self-adjoint)이라는 것이다.
    즉,
    \begin{equation*}
        A = A^t \qquad \Longleftrightarrow \qquad \forall \vec x, \vec y \in \mathbb R^n \quad A\vec x \cdot \vec y = \vec x \cdot A \vec y
    \end{equation*}
    이다.
\end{lemma}

\begin{proof}
    우선 $A$가 대칭행렬이면 스칼라곱에 대해 자기 수반이라는 것을 보이자.

    어떤 $\vec x, \vec y \in \mathbb R^n$을 고르자.
    그러면
    \begin{equation*}
        \vec x \cdot A \vec y = (A\vec y)^t \vec x = \vec y^t A^t \vec x = \vec y^t A \vec x = A \vec x \cdot \vec y
    \end{equation*}
    이므로 $A$는 스칼라곱에 대해 자기 수반이다.

    이제 $A$가 스칼라곱에 대해 자기 수반이면 $A$가 대칭행렬임을 보이자.

    $A = (\vec v_1 \quad \dots \quad \vec v_n) = (a_{ij})$라고 하자.
    $A \vec e_i = \vec v_i$이고, $\vec v_i \cdot \vec e_j = a_{ji}$이다.
    그런데
    \begin{equation*}
        A \vec e_i \cdot e_j = e_i \cdot A \vec e_j
    \end{equation*}
    이므로,
    \begin{equation*}
        a_{ji} = a_{ij}
    \end{equation*}
    이다.
    따라서 $A = A^t$이다.
    그러므로 $A$가 대칭행렬인 것은 $A$가 스칼라곱에 대해 자기 수반인 것과 동치이다.
\end{proof}

\begin{remark}
    $L: \mathcal C^\infty \rightarrow \mathcal C^\infty$에 대해서도 내적 $\langle \cdot, \cdot \rangle$이 주어졌을 때 임의의 $x, y$에 대해 $\langle L x, y \rangle = \langle x, Ly \rangle$인 변환을 생각할 수 있다.
\end{remark}

\begin{theorem} \label{thm:sym_eigenvector_orthogonal}
    대칭 실행렬 $A$의 서로 다른 특성값 $\lambda$와 $\mu$에 대해서, 대응되는 특성벡터 $\vec x$와 $\vec y$는 직교한다.
\end{theorem}

\begin{proof}
    $A$가 대칭 실행렬인 것은 도움정리~\ref{lem:self_adjoint_sym}에 따라 $A$가 스칼라곱에 대해 자기 수반인 것을 의미한다.
    따라서 $\lambda$-특성벡터 $\vec x$와 $\mu$-특성벡터 $\vec y$에 대해
    \begin{equation*}
        A \vec x \cdot \vec y = \vec x \cdot A \vec y
    \end{equation*}
    이며, 따라서
    \begin{equation*}
        \lambda \vec x \cdot \vec y = \vec x \cdot \mu \vec y
    \end{equation*}
    이다.
    우변을 좌변으로 이항하면
    \begin{equation*}
        (\lambda - \mu) \vec x \cdot \vec y = 0
    \end{equation*}
    이고, $\lambda \neq \mu$이므로 $\vec x \cdot \vec y = 0$이다.
\end{proof}

\begin{example}
    $A = \begin{pmatrix}1 & 3\\2 & 4\end{pmatrix} \begin{pmatrix}7 & 0\\ 0 & 8\end{pmatrix}\begin{pmatrix}1 & 3\\2 & 4\end{pmatrix}^t$는 대칭행렬이 아니다.
    $(1, 2) \cdot (3, 4) \neq 0$이기 때문이다.
\end{example}

\begin{remark}
    정리~\ref{thm:sym_eigenvector_orthogonal}에 따라 대칭행렬은 가역 직교행렬을 통해 대각화할 수 있음을 알 수 있다.
    예를 들어, 어떤 행렬 $A$가 다음과 같이 대각화된다고 하자:
    \begin{equation*}
        A =
        \begin{pmatrix}
            \vec v_1 & \vec v_2 & \vec v_3 & \vec w_1 & \vec w_2
        \end{pmatrix}
        \begin{pmatrix}
            2 & 0 & 0 & 0 & 0\\
            0 & 2 & 0 & 0 & 0\\
            0 & 0 & 2 & 0 & 0\\
            0 & 0 & 0 & 3 & 0\\
            0 & 0 & 0 & 0 & 3
        \end{pmatrix}
        \begin{pmatrix}
            \vec v_1^t \\ \vec v_2^t \\ \vec v_3^t \\ \vec w_1^t \\ \vec w_2^t
        \end{pmatrix}
    \end{equation*}
    이 때 $A$의 특성값 2, 3과 2-특성공간 $E_2 = \langle \vec v_1, \vec v_2, \vec v_3 \rangle$과 3-특성공간 $E_3 = \langle \vec w_1, \vec w_2\rangle$을 알 수 있다.
    각각의 특성공간의 기저에 그람-슈미트 과정을 통해서 직교 기저를 얻을 수 있고, 정규화하여 정규직교 기저를 구할 수 있다.
    
    즉, 대칭행렬은 직교 대각화 가능하다:
    대각행렬 $A$에 대해서
    \begin{equation*}
        A = PDP^t \quad \wedge \quad PP^t = I
    \end{equation*}
    인 $P$가 존재한다.
\end{remark}

% \begin{theorem}
%     함수 $\left<\cdot, \cdot\right>: \mathbb R^n \times \mathbb R^n \rightarrow \mathbb R$가 내적이라는 것은 벡터 $\vec a, \vec b \in \mathbb R^n$에 대해 고윳값이 모두 양수인 어떤 $n \times n$ 대칭행렬 $A$가 존재해서 다음을 만족한다는 것과 동치이다:
%     \begin{equation*}
%         \left<\vec a, \vec b\right> = \vec b^t A \vec a
%     \end{equation*}
% \end{theorem}
\end{document}
